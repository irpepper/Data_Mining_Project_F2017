ccopy_reg
_reconstructor
p1
(cpraw.models.reddit.submission
Submission
p2
c__builtin__
object
p3
NtRp4
(dp5
S'domain'
p6
Vjournals.plos.org
p7
sS'approved_at_utc'
p8
NsS'_comments_by_id'
p9
(dp10
sS'_info_params'
p11
(dp12
sS'banned_by'
p13
NsS'comment_sort'
p14
S'best'
p15
sS'media_embed'
p16
(dp17
sS'thumbnail_width'
p18
I140
sS'subreddit'
p19
g1
(cpraw.models.reddit.subreddit
Subreddit
p20
g3
NtRp21
(dp22
S'_wiki'
p23
NsS'display_name'
p24
Vscience
p25
sS'_stylesheet'
p26
NsS'_fetched'
p27
I00
sg11
(dp28
sS'_contributor'
p29
NsS'_banned'
p30
NsS'_muted'
p31
NsS'_quarantine'
p32
NsS'_modmail'
p33
NsS'_flair'
p34
NsS'_reddit'
p35
g1
(cpraw.reddit
Reddit
p36
g3
NtRp37
(dp38
S'_core'
p39
g1
(cprawcore.sessions
Session
p40
g3
NtRp41
(dp42
S'_authorizer'
p43
g1
(cprawcore.auth
ScriptAuthorizer
p44
g3
NtRp45
(dp46
S'scopes'
p47
c__builtin__
set
p48
((lp49
V*
atRp50
sS'_username'
p51
VJS_Research
p52
sS'access_token'
p53
VoR7ueivAi-yeTmPh7RRrUoU-sm8
p54
sS'_authenticator'
p55
g1
(cprawcore.auth
TrustedAuthenticator
p56
g3
NtRp57
(dp58
S'client_secret'
p59
VBgciYaBpyH8ef21jXMyJjn_DARE
p60
sS'redirect_uri'
p61
g1
(cpraw.config
_NotSet
p62
g3
NtRp63
sS'client_id'
p64
VwU1FwN1ugWt7eQ
p65
sS'_requestor'
p66
g1
(cprawcore.requestor
Requestor
p67
g3
NtRp68
(dp69
S'_http'
p70
g1
(crequests.sessions
Session
p71
g3
NtRp72
(dp73
S'cookies'
p74
g1
(crequests.cookies
RequestsCookieJar
p75
g3
NtRp76
(dp77
S'_now'
p78
I1510895610
sS'_policy'
p79
(icookielib
DefaultCookiePolicy
p80
(dp81
S'strict_rfc2965_unverifiable'
p82
I01
sS'strict_ns_domain'
p83
I0
sS'_allowed_domains'
p84
NsS'rfc2109_as_netscape'
p85
NsS'rfc2965'
p86
I00
sS'strict_domain'
p87
I00
sg78
I1510895610
sS'strict_ns_set_path'
p88
I00
sS'strict_ns_unverifiable'
p89
I00
sS'strict_ns_set_initial_dollar'
p90
I00
sS'hide_cookie2'
p91
I00
sS'_blocked_domains'
p92
(tsS'netscape'
p93
I01
sbsS'_cookies'
p94
(dp95
S'.reddit.com'
p96
(dp97
S'/'
(dp98
S'loid'
p99
(icookielib
Cookie
p100
(dp101
S'comment'
p102
Nsg6
S'.reddit.com'
p103
sS'name'
p104
g99
sS'domain_initial_dot'
p105
I00
sS'expires'
p106
I1573967423
sS'value'
p107
S'00000000000e7dsf19.2.1505716153623.Z0FBQUFBQmFEbTlBU1EwbzUyZ293WTJBdDJaamk2V2s2dzRJQmlEV3MxUTQxN212M2ZKZjlhN0xrRnVlSUZoODZzUHZqeEdpdzZ1aFAyT0ZVdUQtRm85dzNpcDNfeG0wbi1iZGs3OWF6WXg4RzNqejI0SV9lUllITXBkUGF4LU14ejUzMWxtLTBHT2Q'
p108
sS'domain_specified'
p109
I01
sS'_rest'
p110
(dp111
sS'version'
p112
I0
sS'port_specified'
p113
I00
sS'rfc2109'
p114
I00
sS'discard'
p115
I00
sS'path_specified'
p116
I01
sS'path'
p117
S'/'
sS'port'
p118
NsS'comment_url'
p119
NsS'secure'
p120
I01
sbsS'session_tracker'
p121
(icookielib
Cookie
p122
(dp123
g102
Nsg6
S'.reddit.com'
p124
sg104
S'session_tracker'
p125
sg105
I00
sg106
I1510902809
sg107
S'crQiBtrEVfCCgtFogt.0.1510895610205.Z0FBQUFBQmFEbV82eHgzMEcwQ0hsRTIwSDVVRTJYcUJCVHJVbkh2a0V2ZnlkWS04a2ZnVkExdW5JQmtxdHN2Z005LXZ1RzZQVGNlSGVPU1JQNmwyMkdxdG9LWGlwNVdKRlQxT0JmTlBxUzludEQtblBxeWViNWNUeEZMSzAxWkVXT1E4TkY3bDl4dDU'
p126
sg109
I01
sg110
(dp127
sg112
I0
sg113
I00
sg114
I00
sg115
I00
sg116
I01
sg117
S'/'
sg118
Nsg119
Nsg120
I01
sbsS'edgebucket'
p128
(icookielib
Cookie
p129
(dp130
g102
Nsg6
S'.reddit.com'
p131
sg104
g128
sg105
I00
sg106
I1573967422
sg107
S'wR7VzApWCkJPkoMBgW'
p132
sg109
I01
sg110
(dp133
sg112
I0
sg113
I00
sg114
I00
sg115
I00
sg116
I01
sg117
S'/'
sg118
Nsg119
Nsg120
I01
sbssssbsS'stream'
p134
I00
sS'hooks'
p135
(dp136
S'response'
p137
(lp138
ssS'auth'
p139
NsS'trust_env'
p140
I01
sS'headers'
p141
g1
(crequests.structures
CaseInsensitiveDict
p142
g3
NtRp143
(dp144
S'_store'
p145
curllib3.packages.ordered_dict
OrderedDict
p146
((lp147
(lp148
S'connection'
p149
a(S'Connection'
p150
S'keep-alive'
p151
tp152
aa(lp153
S'accept-encoding'
p154
a(S'Accept-Encoding'
p155
S'gzip, deflate'
tp156
aa(lp157
S'accept'
p158
a(S'Accept'
p159
S'*/*'
p160
tp161
aa(lp162
S'user-agent'
p163
a(S'User-Agent'
p164
S'News Article Downloader /u/JS_Research PRAW/5.2.0 prawcore/0.12.0'
tp165
aatRp166
sbsS'cert'
p167
NsS'params'
p168
(dp169
sS'prefetch'
p170
NsS'verify'
p171
I01
sS'proxies'
p172
(dp173
sS'adapters'
p174
g146
((lp175
(lp176
S'https://'
p177
ag1
(crequests.adapters
HTTPAdapter
p178
g3
NtRp179
(dp180
S'_pool_block'
p181
I00
sS'_pool_maxsize'
p182
I10
sS'max_retries'
p183
g1
(curllib3.util.retry
Retry
p184
g3
NtRp185
(dp186
S'status'
p187
NsS'redirect'
p188
NsS'read'
p189
I00
sS'backoff_factor'
p190
I0
sS'respect_retry_after_header'
p191
I01
sS'history'
p192
(tsS'raise_on_status'
p193
I01
sS'connect'
p194
NsS'status_forcelist'
p195
g48
((ltRp196
sS'total'
p197
I0
sS'raise_on_redirect'
p198
I01
sS'method_whitelist'
p199
c__builtin__
frozenset
p200
((lp201
S'HEAD'
p202
aS'TRACE'
p203
aS'GET'
p204
aS'PUT'
p205
aS'OPTIONS'
p206
aS'DELETE'
p207
atRp208
sbsS'config'
p209
(dp210
sS'_pool_connections'
p211
I10
sbaa(lp212
S'http://'
p213
ag1
(g178
g3
NtRp214
(dp215
g181
I00
sg182
I10
sg183
g1
(g184
g3
NtRp216
(dp217
g187
Nsg188
Nsg189
I00
sg190
I0
sg191
I01
sg192
(tsg193
I01
sg194
Nsg195
g48
((ltRp218
sg197
I0
sg198
I01
sg199
g208
sbsg209
(dp219
sg211
I10
sbaatRp220
sS'max_redirects'
p221
I30
sbsS'reddit_url'
p222
S'https://www.reddit.com'
p223
sS'oauth_url'
p224
S'https://oauth.reddit.com'
p225
sbsbsS'_password'
p226
VXPlstazuMC0CaX1Njhf0ny5u^nMD5t*P73O
p227
sS'_expiration_timestamp'
p228
F1510899012.1773059
sS'refresh_token'
p229
NsbsS'_rate_limiter'
p230
g1
(cprawcore.rate_limit
RateLimiter
p231
g3
NtRp232
(dp233
S'used'
p234
I114
sS'remaining'
p235
F486
sS'next_request_timestamp'
p236
F1510895611.0569813
sS'reset_timestamp'
p237
F1510896000.2545121
sbsbsS'_objector'
p238
g1
(cpraw.objector
Objector
p239
g3
NtRp240
(dp241
g35
g37
sS'parsers'
p242
(dp243
S'LiveUpdate'
p244
cpraw.models.reddit.live
LiveUpdate
p245
sS'ModmailConversation'
p246
cpraw.models.reddit.modmail
ModmailConversation
p247
sS'ModmailMessage'
p248
cpraw.models.reddit.modmail
ModmailMessage
p249
sS't4'
p250
cpraw.models.reddit.message
Message
p251
sS't5'
p252
g20
sS't2'
p253
cpraw.models.reddit.redditor
Redditor
p254
sS't3'
p255
g2
sS't1'
p256
cpraw.models.reddit.comment
Comment
p257
sS'UserList'
p258
cpraw.models.list.redditor
RedditorList
p259
sS'stylesheet'
p260
cpraw.models.stylesheet
Stylesheet
p261
sS'LabeledMulti'
p262
cpraw.models.reddit.multi
Multireddit
p263
sS'Listing'
p264
cpraw.models.listing.listing
Listing
p265
sS'modaction'
p266
cpraw.models.modaction
ModAction
p267
sS'LiveUpdateEvent'
p268
cpraw.models.reddit.live
LiveThread
p269
sS'ModmailAction'
p270
cpraw.models.reddit.modmail
ModmailAction
p271
sS'more'
p272
cpraw.models.reddit.more
MoreComments
p273
ssbsS'subreddits'
p274
g1
(cpraw.models.subreddits
Subreddits
p275
g3
NtRp276
(dp277
g35
g37
sbsg139
g1
(cpraw.models.auth
Auth
p278
g3
NtRp279
(dp280
g35
g37
sbsg19
g1
(cpraw.models.helpers
SubredditHelper
p281
g3
NtRp282
(dp283
g35
g37
sbsS'front'
p284
g1
(cpraw.models.front
Front
p285
g3
NtRp286
(dp287
g35
g37
sS'_comments'
p288
NsS'_path'
p289
S'/'
sbsS'live'
p290
g1
(cpraw.models.helpers
LiveHelper
p291
g3
NtRp292
(dp293
g35
g37
sbsS'inbox'
p294
g1
(cpraw.models.inbox
Inbox
p295
g3
NtRp296
(dp297
g35
g37
sbsS'multireddit'
p298
g1
(cpraw.models.helpers
MultiredditHelper
p299
g3
NtRp300
(dp301
g35
g37
sbsS'_unique_counter'
p302
I0
sg209
g1
(cpraw.config
Config
p303
g3
NtRp304
(dp305
S'username'
p306
g52
sg222
g223
sS'_settings'
p307
(dp308
g306
g52
sg59
g60
sS'password'
p309
g227
sS'user_agent'
p310
VNews Article Downloader /u/JS_Research
p311
sg64
g65
ssS'check_for_updates'
p312
I01
sS'custom'
p313
(dp314
sg61
g63
sg310
g311
sg64
g65
sS'_short_url'
p315
S'https://redd.it'
p316
sg59
g60
sg224
g225
sg309
g227
sS'kinds'
p317
(dp318
g102
g256
sS'message'
p319
g250
sS'redditor'
p320
g253
sS'submission'
p321
g255
sg19
g252
ssg229
g63
sbsS'_read_only_core'
p322
g1
(g40
g3
NtRp323
(dp324
g43
g1
(cprawcore.auth
ReadOnlyAuthorizer
p325
g3
NtRp326
(dp327
g53
Nsg47
Nsg228
Nsg229
Nsg55
g57
sbsg230
g1
(g231
g3
NtRp328
(dp329
g234
Nsg235
Nsg236
Nsg237
NsbsbsS'_authorized_core'
p330
g41
sS'user'
p331
g1
(cpraw.models.user
User
p332
g3
NtRp333
(dp334
g35
g37
sS'_me'
p335
g1
(g254
g3
NtRp336
(dp337
S'is_employee'
p338
I00
sS'has_visited_new_profile'
p339
I00
sS'pref_no_profanity'
p340
I01
sg11
(dp341
sS'is_suspended'
p342
I00
sS'pref_geopopular'
p343
V
sS'_listing_use_sort'
p344
I01
sg19
NsS'is_sponsor'
p345
I00
sS'gold_expiration'
p346
NsS'id'
p347
Ve7dsf19
p348
sS'suspension_expiration_utc'
p349
NsS'verified'
p350
I00
sg27
I00
sS'new_modmail_exists'
p351
NsS'features'
p352
(dp353
Vsearch_public_traffic
p354
(dp355
Vowner
p356
Vsearch
p357
sVvariant
p358
Vnew_search_11
p359
sVexperiment_id
p360
I212
ssVdo_not_track
p361
I01
sVgeopopular_au_holdout
p362
(dp363
Vowner
p364
Vrelevance
p365
sVvariant
p366
Vcontrol_2
p367
sVexperiment_id
p368
I206
ssVshow_amp_link
p369
I01
sVlive_happening_now
p370
I01
sVadserver_reporting
p371
I01
sVgeopopular
p372
I01
sVchat_rollout
p373
I01
sVads_auto_refund
p374
I01
sVlisting_service_rampup
p375
I01
sVmobile_web_targeting
p376
I01
sVdefault_srs_holdout
p377
(dp378
Vowner
p379
Vrelevance
p380
sVvariant
p381
Vtutorial
p382
sVexperiment_id
p383
I171
ssVadzerk_do_not_track
p384
I01
sVusers_listing
p385
I01
sVshow_user_sr_name
p386
I01
sVwhitelisted_pms
p387
I01
sVpersonalization_prefs
p388
I01
sVupgrade_cookies
p389
I01
sVnew_overview
p390
I01
sVnew_report_flow
p391
I01
sVblock_user_by_report
p392
I01
sVadblock_test
p393
I01
sVlegacy_search_pref
p394
I01
sVorangereds_as_emails
p395
I01
sVmweb_xpromo_modal_listing_click_daily_dismissible_ios
p396
I01
sVexpando_events
p397
I01
sVeu_cookie_policy
p398
I01
sVprogrammatic_ads
p399
I01
sVforce_https
p400
I01
sVinbox_push
p401
I01
sVpost_to_profile_beta
p402
I01
sVcrossposting_ga
p403
I01
sVoutbound_clicktracking
p404
I01
sVnew_loggedin_cache_policy
p405
I01
sVshow_secret_santa
p406
I01
sVhttps_redirect
p407
I01
sVsearch_dark_traffic
p408
I01
sVmweb_xpromo_interstitial_comments_ios
p409
I01
sVpause_ads
p410
I01
sVgive_hsts_grants
p411
I01
sVshow_recommended_link
p412
I01
sVmobile_native_banner
p413
I01
sVmweb_xpromo_interstitial_comments_android
p414
I01
sVads_auction
p415
I01
sVgeopopular_se_holdout
p416
(dp417
Vowner
p418
Vrelevance
p419
sVvariant
p420
Vcontrol_2
p421
sVexperiment_id
p422
I224
ssVscreenview_events
p423
I01
sVsubreddit_recommendations_carousel_holdout
p424
(dp425
Vowner
p426
Vrelevance
p427
sVvariant
p428
Vcontrol_2
p429
sVexperiment_id
p430
I239
ssVnew_report_dialog
p431
I01
sVmoat_tracking
p432
I01
sVsubreddit_rules
p433
I01
sVadzerk_reporting_2
p434
I01
sVactivity_service_write
p435
I01
sVads_auto_extend
p436
I01
sVinterest_targeting
p437
I01
sVpost_embed
p438
I01
sVmweb_xpromo_ad_loading_android
p439
(dp440
Vowner
p441
Vchannels
p442
sVvariant
p443
Vcontrol_1
p444
sVexperiment_id
p445
I187
ssVscroll_events
p446
I01
sVmweb_xpromo_modal_listing_click_daily_dismissible_android
p447
I01
sVcrossposting_recent
p448
I01
sVactivity_service_read
p449
I01
ssS'over_18'
p450
I00
sS'is_gold'
p451
I00
sS'is_mod'
p452
I00
sS'has_verified_email'
p453
I00
sS'in_redesign_beta'
p454
I00
sS'has_mod_mail'
p455
I00
sS'oauth_client_id'
p456
VwU1FwN1ugWt7eQ
p457
sS'hide_from_robots'
p458
I00
sS'link_karma'
p459
I1
sg35
g37
sS'inbox_count'
p460
I1
sS'pref_top_karma_subreddits'
p461
NsS'has_mail'
p462
I01
sS'pref_show_snoovatar'
p463
I00
sg104
VJS_Research
p464
sS'created'
p465
F1505744953
sS'_stream'
p466
NsS'gold_creddits'
p467
I0
sS'created_utc'
p468
F1505716153
sS'in_beta'
p469
I00
sS'comment_karma'
p470
I0
sS'has_subscribed'
p471
I00
sg289
S'user/JS_Research/'
p472
sbsbsbsS'_filters'
p473
Nsg466
Nsg288
NsS'_mod'
p474
NsS'_moderator'
p475
Nsg289
S'r/science/'
p476
sbsS'selftext_html'
p477
NsS'selftext'
p478
V
sS'likes'
p479
NsS'suggested_sort'
p480
Vconfidence
p481
sS'user_reports'
p482
(lp483
sS'secure_media'
p484
NsS'is_reddit_media_domain'
p485
I00
sS'link_flair_text'
p486
VComputer Science
p487
sg347
V5vuvz9
p488
sS'banned_at_utc'
p489
NsS'view_count'
p490
NsS'archived'
p491
I01
sS'clicked'
p492
I00
sS'report_reasons'
p493
NsS'title'
p494
VThe online world has turned into an ecosystem of bots. Oxford researchers analyze the interactions between bots that edit articles on Wikipedia. Their findings suggest that even relatively \u201cdumb\u201d bots may give rise to complex interactions, and this carries important implications for AI research.
p495
sS'num_crossposts'
p496
I0
sS'saved'
p497
I00
sS'mod_reports'
p498
(lp499
sS'can_mod_post'
p500
I00
sS'is_crosspostable'
p501
I00
sS'pinned'
p502
I00
sS'comment_limit'
p503
I2048
sS'score'
p504
I263
sS'approved_by'
p505
Nsg450
I00
sS'hidden'
p506
I00
sS'preview'
p507
(dp508
Vimages
p509
(lp510
(dp511
Vsource
p512
(dp513
Vurl
p514
Vhttps://i.redditmedia.com/alGGDyWIXEWwFbqtiDdgWOi_ju1OhBseBTsg_F5krGY.jpg?s=ffeae59160dc7abce37d375088a50b1d
p515
sVwidth
p516
I320
sVheight
p517
I173
ssVresolutions
p518
(lp519
(dp520
Vurl
p521
Vhttps://i.redditmedia.com/alGGDyWIXEWwFbqtiDdgWOi_ju1OhBseBTsg_F5krGY.jpg?fit=crop&crop=faces%2Centropy&arh=2&w=108&s=51ae0ea785ab12c2ccf2154eb01314b6
p522
sVwidth
p523
I108
sVheight
p524
I58
sa(dp525
Vurl
p526
Vhttps://i.redditmedia.com/alGGDyWIXEWwFbqtiDdgWOi_ju1OhBseBTsg_F5krGY.jpg?fit=crop&crop=faces%2Centropy&arh=2&w=216&s=626da194c06a05dbc740d80c5762c166
p527
sVwidth
p528
I216
sVheight
p529
I116
sa(dp530
Vurl
p531
Vhttps://i.redditmedia.com/alGGDyWIXEWwFbqtiDdgWOi_ju1OhBseBTsg_F5krGY.jpg?fit=crop&crop=faces%2Centropy&arh=2&w=320&s=4e0336d4a3dbf2876e67f7c57f816f86
p532
sVwidth
p533
I320
sVheight
p534
I173
sasVvariants
p535
(dp536
sVid
p537
VjZTr_LqzsV1fmMNAvu7qqwkfLRre434Se1eDTJ7tRUw
p538
sasVenabled
p539
I00
ssS'thumbnail'
p540
Vhttps://b.thumbs.redditmedia.com/1aQrRsQ8P2w6PBaEsX6V1CzL0q3WKM6x11IcxDmwJCA.jpg
p541
sS'subreddit_id'
p542
Vt5_mouw
p543
sS'whitelist_status'
p544
Vall_ads
p545
sS'edited'
p546
I00
sS'link_flair_css_class'
p547
Vcompsci
p548
sS'author_flair_css_class'
p549
Vmed   reward3
p550
sS'contest_mode'
p551
I00
sS'gilded'
p552
I0
sS'downs'
p553
I0
sS'brand_safe'
p554
I01
sS'secure_media_embed'
p555
(dp556
sS'removal_reason'
p557
NsS'post_hint'
p558
Vlink
p559
sS'stickied'
p560
I00
sg35
g37
sS'can_gild'
p561
I01
sS'thumbnail_height'
p562
I75
sS'parent_whitelist_status'
p563
Vall_ads
p564
sg104
Vt3_5vuvz9
p565
sg474
NsS'spoiler'
p566
I00
sS'permalink'
p567
V/r/science/comments/5vuvz9/the_online_world_has_turned_into_an_ecosystem_of/
p568
sS'subreddit_type'
p569
Vpublic
p570
sS'locked'
p571
I00
sS'hide_score'
p572
I00
sg465
F1487931807
sS'url'
p573
Vhttp://journals.plos.org/plosone/article?id=10.1371/journal.pone.0171774
p574
sS'author_flair_text'
p575
VMD-PhD-MBA | Clinical Professor/Medicine
p576
sS'quarantine'
p577
I00
sS'author'
p578
g1
(g254
g3
NtRp579
(dp580
g104
Vmvea
p581
sg27
I00
sg11
(dp582
sg344
I01
sg466
Nsg35
g37
sg289
S'user/mvea/'
p583
sbsg468
F1487903007
sS'subreddit_name_prefixed'
p584
Vr/science
p585
sS'ups'
p586
I263
sg34
NsS'media'
p587
NsS'article_text'
p588
VAbstract In recent years, there has been a huge increase in the number of bots online, varying from Web crawlers for search engines, to chatbots for online customer service, spambots on social media, and content-editing bots in online collaboration communities. The online world has turned into an ecosystem of bots. However, our knowledge of how these automated agents are interacting with each other is rather poor. Bots are predictable automatons that do not have the capacity for emotions, meaning-making, creativity, and sociality and it is hence natural to expect interactions between bots to be relatively predictable and uneventful. In this article, we analyze the interactions between bots that edit articles on Wikipedia. We track the extent to which bots undid each others edits over the period 20012010, model how pairs of bots interact over time, and identify different types of interaction trajectories. We find that, although Wikipedia bots are intended to support the encyclopedia, they often undo each others edits and these sterile fights may sometimes continue for years. Unlike humans on Wikipedia, bots interactions tend to occur over longer periods of time and to be more reciprocated. Yet, just like humans, bots in different cultural environments may behave differently. Our research suggests that even relatively dumb bots may give rise to complex interactions, and this carries important implications for Artificial Intelligence research. Understanding what affects bot-bot interactions is crucial for managing social media well, providing adequate cyber-security, and designing well functioning autonomous vehicles.\u000a\u000aCitation: Tsvetkova M, Garca-Gavilanes R, Floridi L, Yasseri T (2017) Even good bots fight: The case of Wikipedia. PLoS ONE12(2): e0171774. https://doi.org/10.1371/journal.pone.0171774 Editor: Sergio Gmez, Universitat Rovira i Virgili, SPAIN Received: November 16, 2016; Accepted: January 25, 2017; Published: February 23, 2017 Copyright:  2017 Tsvetkova et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Data Availability: All relevant data are available from figshare at 10.6084/m9.figshare.4597918. Funding: This work has received funding from the European Unions Horizon 2020 research and innovation program under grant agreement No. 645043: HUMANE. Competing interests: The authors have declared that no competing interests exist.\u000a\u000aIntroduction In August 2011, Igor Labutov and Jason Yosinski, two PhD students at Cornell University, let a pair of chat bots, called Alan and Sruthi, talk to each other online. Starting with a simple greeting, the one-and-a-half-minute dialogue quickly escalated into an argument about what Alan and Sruthi had just said, whether they were robots, and about God [1]. The first ever conversation between two simple artificial intelligence agents ended in a conflict. A bot, or software agent, is a computer program that is persistent, autonomous, and reactive [2,3]. Bots are defined by programming code that runs continuously and can be activated by itself. They make and execute decisions without human intervention and perceive and adapt to the context they operate in. Internet bots, also known as web bots, are bots that run over the Internet. They appeared and proliferated soon after the creation of the World Wide Web [4]. Already in 1993, Martijn Koster published Guidelines to robot writers, which contained suggestions about developing web crawlers [5], a kind of bot. Eggdrop, one of the first known Internet Relay Chat bots, started greeting chat newcomers also in 1993 [6]. In 1996, Fah-Chun Cheong published a 413-page book, claiming to have a current listing of all bots available on the Internet at that point in time. Since then, Internet bots have proliferated and diversified well beyond our ability to record them in an exhaustive list [7,8]. As a result, bots have been responsible for an increasingly larger proportion of activities on the Web. For example, one study found that 25% of all messages on Yahoo! chat over a period of three months in 2007 were sent by spam bots [9]. Another study discovered that 32% of all tweets made by the most active Twitter users in 2009 were generated by bots [10], meaning that bots were responsible for an estimated 24% of all tweets [11]. Further, researchers estimated that bots comprise between 4% and 7% of the avatars on the virtual world Second Life in 2009 [12]. A media analytics company found that 54% of the online ads shown in thousands of ad campaigns in 2012 and 2013 were viewed by bots, rather than humans [13]. According to an online security company, bots accounted for 48.5% of website visits in 2015 [14]. Also in 2015, 100,000 accounts on the multi-player online game World of Warcraft (about 1% of all accounts) were banned for using bots [15]. And in the same year, a database leak revealed that more than 70,000 female bots sent more than 20 million messages on the cheater dating site Ashley Madison [16]. As the population of bots active on the Internet 24/7 is growing fast, their interactions are equally intensifying. An increasing number of decisions, options, choices, and services depend now on bots working properly, efficaciously, and successfully. Yet, we know very little about the life and evolution of our digital minions. In particular, predicting how bots interactions will evolve and play out even when they rely on very simple algorithms is already challenging. Furthermore, as Alan and Sruthi demonstrated, even if bots are designed to collaborate, conflict may occur inadvertently. Clearly, it is crucial to understand what could affect bot-bot interactions in order to design cooperative bots that can manage disagreement, avoid unproductive conflict, and fulfill their tasks in ways that are socially and ethically acceptable. There are many types of Internet bots (see Table 1). These bots form an increasingly complex system of social interactions. Do bots interact with each other in ways that are comparable to how we humans interact with each other? Bots are predictable automatons that do not have the capacity for emotions, meaning-making, creativity, and sociality [17]. Despite recent advances in the field of Artificial Intelligence, the idea that bots can have morality and culture is still far from reality. Today, it is natural to expect interactions between bots to be relatively predictable and uneventful, lacking the spontaneity and complexity of human social interactions. However, even in such simple contexts, our research shows that there may be more similarities between bots and humans than one may expect. Focusing on one particular human-bot community, we find that conflict emerges even among benevolent bots that are designed to benefit their environment and not fight each other, and that bot interactions may differ when they occur in environments influenced by different human cultures. PPT PowerPoint slide\u000a\u000aPowerPoint slide PNG larger image\u000a\u000alarger image TIFF original image Download: Table 1. Categorization of Internet bots according to the intended effect of their operations and the kind of activities they perform, including some familiar examples for each type. https://doi.org/10.1371/journal.pone.0171774.t001 Benevolent bots are designed to support human users or cooperate with them. Malevolent bots are designed to exploit human users and compete negatively with them. We have classified high-frequency trading algorithms as malevolent because they exploit markets in ways that increase volatility and precipitate flash crashes. In this study, we use data from editing bots on Wikipedia (benevolent bots that generate content). We study bots on Wikipedia, the largest free online encyclopedia. Bots on Wikipedia are computer scripts that automatically handle repetitive and mundane tasks to develop, improve, and maintain the encyclopedia. They are easy to identify because they operate from dedicated user accounts that have been flagged and officially approved. Approval requires that the bot follows Wikipedias bot policy. Bots are important contributors to Wikipedia. For example, in 2014, bots completed about 15% of the edits on all language editions of the encyclopedia [18]. In general, Wikipedia bots complete a variety of activities. They identify and undo vandalism, enforce bans, check spelling, create inter-language links, import content automatically, mine data, identify copyright violations, greet newcomers, and so on [19]. Our analysis here focuses on editing bots, which modify articles directly. We analyze the interactions between bots and investigate the extent to which they resemble interactions between humans. In particular, we focus on whether bots disagree with each other, how the dynamics of disagreement differ for bots versus humans, and whether there are differences between bots operating in different language editions of Wikipedia. To measure disagreement, we study reverts. A revert on Wikipedia occurs when an editor, whether human or bot, undoes another editors contribution by restoring an earlier version of the article. Reverts that occur systematically indicate controversy and conflict [2022]. Reverts are technically easy to detect regardless of the context and the language, so they enable analysis at the scale of the whole system. Our data contain all edits in 13 different language editions of Wikipedia in the first ten years after the encyclopedia was launched (20012010). The languages represent editions of different size and editors from diverse cultures (see Materials and Methods for details). We know which user completed the edit, when, in which article, whether the edit was a revert and, if so, which previous edit was reverted. We first identified which editors are humans, bots, or vandals. We isolated the vandals since their short-lived disruptive activity exhibits different time and interaction patterns than the activity of regular Wikipedia editors.\u000a\u000aDiscussion Our results show that, although in quantitatively different ways, bots on Wikipedia behave and interact as unpredictably and as inefficiently as the humans. The disagreements likely arise from the bottom-up organization of the community, whereby human editors individually create and run bots, without a formal mechanism for coordination with other bot owners. Delving deeper into the data, we found that most of the disagreement occurs between bots that specialize in creating and modifying links between different language editions of the encyclopedia. The lack of coordination may be due to different language editions having slightly different naming rules and conventions. In support of this argument, we also found that the same bots are responsible for the majority of reverts in all the language editions we study. For example, some of the bots that revert the most other bots include Xqbot, EmausBot, SieBot, and VolkovBot, all bots specializing in fixing inter-wiki links. Further, while there are few articles with many bot-bot reverts (S7 Fig), these articles tend to be the same across languages. For example, some of the articles most contested by bots are about Pervez Musharraf (former president of Pakistan), Uzbekistan, Estonia, Belarus, Arabic language, Niels Bohr, Arnold Schwarzenegger. This would suggest that a significant portion of bot-bot fighting occurs across languages rather than within. In contrast, the articles with most human-human reverts tend to concern local personalities and entities and tend to be unique for each language [26]. Our data cover a period of the evolution of Wikipedia when bot activity was growing. Evidence suggests that this period suddenly ended in 2013 (http://stats.wikimedia.org/EN/PlotsPngEditHistoryTop.htm). This decline occurred because at the beginning of 2013 many language editions of Wikipedia started to provide inter-language links via Wikidata, which is a collaboratively edited knowledge base intended to support Wikipedia. Since our results were largely dictated by inter-language bots, we believe that the conflict we observed on Wikipedia no longer occurs today. One interesting direction for future research is to investigate whether the conflict continues to persist among the inter-language bots that migrated to Wikidata. Wikipedia is perhaps one of the best examples of a populous and complex bot ecosystem but this does not necessarily make it representative. As Table 1 demonstrates, we have investigated a very small region of the botosphere on the Internet. The Wikipedia bot ecosystem is gated and monitored and this is clearly not the case for systems of malevolent social bots, such as social bots on Twitter posing as humans to spread political propaganda or influence public discourse. Unlike the benevolent but conflicting bots of Wikipedia, many malevolent bots are collaborative, often coordinating their behavior as part of botnets [27]. However, before being able to study the social interactions of these bots, we first need to learn to identify them [28]. Our analysis shows that a system of simple bots may produce complex dynamics and unintended consequences. In the case of Wikipedia, we see that benevolent bots that are designed to collaborate may end up in continuous disagreement. This is both inefficient as a waste of resources, and inefficacious, for it may lead to local impasse. Although such disagreements represent a small proportion of the bots editorial activity, they nevertheless bring attention to the complexity of designing artificially intelligent agents. Part of the complexity stems from the common field of interactionbots on the Internet, and in the world at large, do not act in isolation, and interaction is inevitable, whether designed for or not. Part of the complexity stems from the fact that there is a human designer behind every bot, as well as behind the environment in which bots operate, and that human artifacts embody human culture. As bots continue to proliferate and become more sophisticated, social scientist will need to devote more attention to understanding their culture and social life.\u000a\u000aMaterials and methods Data Wikipedia is an ecosystem of bots. Some of the bots are editing bots, that work on the articles. They undo vandalism, enforce bans, check spelling, create inter-language links, import content automatically, etc. Other bots are non-editing: these bots mine data, identify vandalism, or identify copyright violations. In addition to bots, there are also certain automated services that editors use to streamline their work. For example, there are automated tools such Huggle and STiki, which produce a filtered set of edits to review in a live queue. Using these tools, editors can instantly revert the edit in question with a single click and advance to the next one. There are also user interface extensions and in-browser functions such as Twinkle, rollback, and undo, which also allow editors to revert with a single click. Another automated service that is relatively recent and much more sophisticated is the Objective Revision Evaluation Service (ORES). It uses machine-learning techniques to rank edits with the ultimate goal to identify vandals or low-quality contributions. Our research focuses on editing bots. Our data contain who reverts whom, when, and in what article. To obtain this information, we analyzed the Wikipedia XML Dumps (https://dumps.wikimedia.org/mirrors.html) of 13 different language editions. To detect restored versions of an article, a hash was calculated for the complete article text following each revision and the hashes were compared between revisions [23]. The data cover the period from the beginning of Wikipedia (January 15, 2001) until February 2, 2010 October 31, 2011, the last date depending on when the data was collected for the particular language edition. This time period captures the first generation of Wikipedia bots, as in later years, Wikidata took over some of the tasks previously controlled by Wikipedia. The sample of languages covers a wide range of Wikipedia editions in terms of size; for example, it includes the four largest editions by number of edits and number of editors. In terms of cultural diversity, the sample covers a wide range of geographies. Wikipedia requires that human editors create separate accounts for bots and that the bot account names clearly indicate the user is a bot, usually by including the word bot (https://en.wikipedia.org/wiki/Wikipedia:Bot_policy). Hence, to identify the bots, we selected all account names that contain different spelling variations of the word bot. We supplemented this set with all accounts that have currently active bot status in the Wikipedia database but that may not fit the above criterion (using https://en.wikipedia.org/wiki/Wikipedia:Bots/Status as of August 6, 2015). We thus obtained a list of 6,627 suspected bots. We then used the Wikipedia API to check the User page for each suspected bot account. If the page contained a link to another account, we confirmed that the current account was a bot and linked it to its owner. For pages that contained zero or more than one links to other accounts, we manually checked the User and User_talk pages for the suspected bot account to see if it is indeed a bot and to identify its owner. The majority of manually checked accounts were vandals or humans, so we ended up with 1,549 bots, each linked to its human owner. We additionally labeled human editors as vandals if they had all their edits reverted by others. This rule meant that we labeled as vandals also newcomers who became discouraged and left Wikipedia after all their initial contributions were reverted. Since we are interested in social interactions emerging from repeated activity, we do not believe that this decision affects our results. Using the revert data, we created a directed two-layer multi-edge network, where ownership couples the layer of human editors and the layer of bots [29]. To build the network, we assumed that a link goes from the editor who restored an earlier version of the article (the reverter) to the editor who made the revision immediately after that version (the reverted). All links were time-stamped. We collapsed multiple bots to a single node if they were owned by the same human editor; these bots were usually accounts for different generations of the same bot with the same function. In the network, reverts can be both intra- and inter-layer: they occur within the human layer, within the bot layer, and in either direction between the human and bot layers. The multi-layer network was pruned by removing self-reverts, as well as reverts between a bot and its owner. Interaction trajectories We model the interaction trajectories in two-dimensional space, where the x-axis measures time and the y-axis measures the difference between the number of times i has reverted j and the number of times j has reverted i. To construct the trajectories, starting from y 0 = 0, y t = y t-1 + 1 if i reverts j at time t and y t = y t-1  1 if j reverts i at time t; the labels i and j are assigned so that y >= 0 for the majority of the ij interaction time. We analyze three properties of the trajectories: Latency. We define latency as the mean log time in seconds between successive reverts: (log 10 t).\u000a\u000at). Imbalance. We define imbalance as the final proportion of reverts between i and j that were not reciprocated: |r i  r j | / (r i + r j ), where r i and r j are the number of times i reverted j and j reverted i, respectively.\u000a\u000a r | / (r + r ), where r and r are the number of times i reverted j and j reverted i, respectively. Reciprocity. We define reciprocity as the proportion of observed turning points out of all possible: (# turning points) / (r i + r j  1), where r i and r j are the number of times i reverted j and j reverted i, respectively. A turning point occurs when the user who reverts at time t is different from the user who reverts at time t+1. K-means clustering To identify the number of clusters k that best represents the data, we apply the elbow and silhouette methods on trajectories of different minimum length. The rationale behind restricting the data to long trajectories only is that short trajectories tend to have extreme values on the three features, thus possibly skewing the results. According to the elbow method, we would like the smallest k that most significantly reduces the sum of squared errors for the clustering. According to the silhouette method, we would like the k that maximizes the separation distance between clusters and thus gives us the largest silhouette score. Although the elbow method suggests that four clusters provide the best clustering, the silhouette method indicates that the data cannot be clustered well (S8 Fig). We do not necessarily expect that trajectories cluster naturally; rather, we employ clustering in order to quantify the differences between the interactions of bots versus humans across languages. We hence analyze the clustering with k = 4. This clustering also has the advantage of yielding four types of trajectories that intuitively make sense.\u000a\u000aSupporting information S1 Fig. The number of bots, the number of edits by bots, and the proportion of edits done by bots between 2001 and 2010. Between 2003 and 2008 the number of bots and their activity have been increasing. This trend, however, appears to have subsided after 2008, suggesting that the system may have stabilized. https://doi.org/10.1371/journal.pone.0171774.s001 (TIFF) S2 Fig. For the majority of languages, bots are mainly reverted by other bots, as opposed to human editors or vandals. English and the Romance languages in our data present exceptions, with less than 20% of bot reverts are done by other bots. https://doi.org/10.1371/journal.pone.0171774.s002 (TIFF) S3 Fig. Bot-bot interactions have different characteristic time scale than human-human interactions. The figures show the distribution of interactions for a particular latency, where we define latency as the mean log time in seconds between successive reverts. (A) Bot-bot interactions have a characteristic latency of 1 month, as indicated by the peak in the figure. (B) Human-human interactions occur with a latency of 2 minutes, 24 hours, or 1 year. https://doi.org/10.1371/journal.pone.0171774.s003 (TIFF) S4 Fig. Bot-bot interactions are on average more balanced than human-human interactions. We define imbalance as the final proportion of reverts between i and j that were not reciprocated. (A) A significant proportion of bot-bot interactions have low imbalance. (B) The majority of human-human interactions are perfectly unbalanced. https://doi.org/10.1371/journal.pone.0171774.s004 (TIFF) S5 Fig. Bots reciprocate much more than humans do also at a smaller timescale. We measure reciprocity as the proportion of observed turning points out of all possible. (A) A significant proportion of bot-bot interactions have intermediate or high values of reciprocity. (B) The majority of human-human interactions are not reciprocated. https://doi.org/10.1371/journal.pone.0171774.s005 (TIFF) S6 Fig. Four types of interaction trajectories suggested by the k-means analysis. The left panels show a sample of the trajectories, including bot-bot and human-human interactions and trajectories from all languages. The right panels show the distribution of latency, imbalance, and reciprocity for each type of trajectory. The three properties measure the average steepness, the y-value of the last point, and the jaggedness of the trajectory, respectively. (A) Fast unbalanced trajectories have low reciprocity and latency and high imbalance. (B) Somewhat balanced trajectories have intermediate imbalance and reciprocity. (C) Slow unbalanced trajectories have low reciprocity and high latency and imbalance. (D) Well balanced trajectories have low imbalance and high reciprocity. https://doi.org/10.1371/journal.pone.0171774.s006 (TIFF) S7 Fig. The number of articles with a certain number of bot-bot and human-human reverts. (A) Few articles include more than 10 bot-bot reverts. The most contested articles tend to be about foreign countries and personalities. Further, the same articles also re-appear in different languages. (B) There are many articles that are highly contested by humans. The most contested articles tend to concern local personalities and entities. It is rare that a highly contested article in one language will be also highly contested in another language. https://doi.org/10.1371/journal.pone.0171774.s007 (TIFF) S8 Fig. Performance of the k-means clustering algorithm for different number of clusters and for sub-samples with different minimum length of trajectories. (A) The elbow method requires the smallest k that most significantly reduces the sum of squared errors for the clustering. Here, the method suggests that four clusters give the best clustering of the data. (B) The silhouette method requires the k that maximizes the separation distance between clusters, i.e. the largest silhouette score. Here, the method suggests that the clustering performs worse as the number of clusters increases. https://doi.org/10.1371/journal.pone.0171774.s008 (TIFF) S1 Table. Descriptive statistics for the bot-bot layer and the human-human layer in the multi-layer networks of reverts. Bots revert each other to a great extent. They also reciprocate each others reverts to a considerable extent. Their interactions are not as clustered as for human editors. Still, both for bots and humans, more senior editors tend to revert less senior editors, as measured by node assortativity by number of edits completed. https://doi.org/10.1371/journal.pone.0171774.s009 (PDF)\u000a\u000aAcknowledgments The authors thank Wikimedia Deutchland e.V. and Wikimedia Foundation for the live access to the Wikipedia data via Toolserver. The data reported in the paper are available at 10.6084/m9.figshare.4597918.\u000a\u000aAuthor Contributions Conceptualization: MT LF TY. Data curation: MT RG TY. Formal analysis: MT. Funding acquisition: TY. Investigation: MT. Methodology: MT TY. Project administration: TY. Resources: TY. Supervision: TY. Visualization: MT. Writing  original draft: MT. Writing  review & editing: MT RG LF TY.
p589
sS'num_comments'
p590
I8
sS'is_self'
p591
I00
sS'visited'
p592
I00
sS'num_reports'
p593
NsS'is_video'
p594
I00
sS'distinguished'
p595
Nsg27
I00
sb.