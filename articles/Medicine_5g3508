ccopy_reg
_reconstructor
p1
(cpraw.models.reddit.submission
Submission
p2
c__builtin__
object
p3
NtRp4
(dp5
S'domain'
p6
Vjamanetwork.com
p7
sS'approved_at_utc'
p8
NsS'_comments_by_id'
p9
(dp10
sS'_info_params'
p11
(dp12
sS'banned_by'
p13
NsS'comment_sort'
p14
S'best'
p15
sS'media_embed'
p16
(dp17
sS'thumbnail_width'
p18
I140
sS'subreddit'
p19
g1
(cpraw.models.reddit.subreddit
Subreddit
p20
g3
NtRp21
(dp22
S'_wiki'
p23
NsS'display_name'
p24
Vscience
p25
sS'_stylesheet'
p26
NsS'_fetched'
p27
I00
sg11
(dp28
sS'_contributor'
p29
NsS'_banned'
p30
NsS'_muted'
p31
NsS'_quarantine'
p32
NsS'_modmail'
p33
NsS'_flair'
p34
NsS'_reddit'
p35
g1
(cpraw.reddit
Reddit
p36
g3
NtRp37
(dp38
S'_core'
p39
g1
(cprawcore.sessions
Session
p40
g3
NtRp41
(dp42
S'_authorizer'
p43
g1
(cprawcore.auth
ScriptAuthorizer
p44
g3
NtRp45
(dp46
S'scopes'
p47
c__builtin__
set
p48
((lp49
V*
atRp50
sS'_username'
p51
VJS_Research
p52
sS'access_token'
p53
VoR7ueivAi-yeTmPh7RRrUoU-sm8
p54
sS'_authenticator'
p55
g1
(cprawcore.auth
TrustedAuthenticator
p56
g3
NtRp57
(dp58
S'client_secret'
p59
VBgciYaBpyH8ef21jXMyJjn_DARE
p60
sS'redirect_uri'
p61
g1
(cpraw.config
_NotSet
p62
g3
NtRp63
sS'client_id'
p64
VwU1FwN1ugWt7eQ
p65
sS'_requestor'
p66
g1
(cprawcore.requestor
Requestor
p67
g3
NtRp68
(dp69
S'_http'
p70
g1
(crequests.sessions
Session
p71
g3
NtRp72
(dp73
S'cookies'
p74
g1
(crequests.cookies
RequestsCookieJar
p75
g3
NtRp76
(dp77
S'_now'
p78
I1510895610
sS'_policy'
p79
(icookielib
DefaultCookiePolicy
p80
(dp81
S'strict_rfc2965_unverifiable'
p82
I01
sS'strict_ns_domain'
p83
I0
sS'_allowed_domains'
p84
NsS'rfc2109_as_netscape'
p85
NsS'rfc2965'
p86
I00
sS'strict_domain'
p87
I00
sg78
I1510895610
sS'strict_ns_set_path'
p88
I00
sS'strict_ns_unverifiable'
p89
I00
sS'strict_ns_set_initial_dollar'
p90
I00
sS'hide_cookie2'
p91
I00
sS'_blocked_domains'
p92
(tsS'netscape'
p93
I01
sbsS'_cookies'
p94
(dp95
S'.reddit.com'
p96
(dp97
S'/'
(dp98
S'loid'
p99
(icookielib
Cookie
p100
(dp101
S'comment'
p102
Nsg6
S'.reddit.com'
p103
sS'name'
p104
g99
sS'domain_initial_dot'
p105
I00
sS'expires'
p106
I1573967423
sS'value'
p107
S'00000000000e7dsf19.2.1505716153623.Z0FBQUFBQmFEbTlBU1EwbzUyZ293WTJBdDJaamk2V2s2dzRJQmlEV3MxUTQxN212M2ZKZjlhN0xrRnVlSUZoODZzUHZqeEdpdzZ1aFAyT0ZVdUQtRm85dzNpcDNfeG0wbi1iZGs3OWF6WXg4RzNqejI0SV9lUllITXBkUGF4LU14ejUzMWxtLTBHT2Q'
p108
sS'domain_specified'
p109
I01
sS'_rest'
p110
(dp111
sS'version'
p112
I0
sS'port_specified'
p113
I00
sS'rfc2109'
p114
I00
sS'discard'
p115
I00
sS'path_specified'
p116
I01
sS'path'
p117
S'/'
sS'port'
p118
NsS'comment_url'
p119
NsS'secure'
p120
I01
sbsS'session_tracker'
p121
(icookielib
Cookie
p122
(dp123
g102
Nsg6
S'.reddit.com'
p124
sg104
S'session_tracker'
p125
sg105
I00
sg106
I1510902809
sg107
S'crQiBtrEVfCCgtFogt.0.1510895610205.Z0FBQUFBQmFEbV82eHgzMEcwQ0hsRTIwSDVVRTJYcUJCVHJVbkh2a0V2ZnlkWS04a2ZnVkExdW5JQmtxdHN2Z005LXZ1RzZQVGNlSGVPU1JQNmwyMkdxdG9LWGlwNVdKRlQxT0JmTlBxUzludEQtblBxeWViNWNUeEZMSzAxWkVXT1E4TkY3bDl4dDU'
p126
sg109
I01
sg110
(dp127
sg112
I0
sg113
I00
sg114
I00
sg115
I00
sg116
I01
sg117
S'/'
sg118
Nsg119
Nsg120
I01
sbsS'edgebucket'
p128
(icookielib
Cookie
p129
(dp130
g102
Nsg6
S'.reddit.com'
p131
sg104
g128
sg105
I00
sg106
I1573967422
sg107
S'wR7VzApWCkJPkoMBgW'
p132
sg109
I01
sg110
(dp133
sg112
I0
sg113
I00
sg114
I00
sg115
I00
sg116
I01
sg117
S'/'
sg118
Nsg119
Nsg120
I01
sbssssbsS'stream'
p134
I00
sS'hooks'
p135
(dp136
S'response'
p137
(lp138
ssS'auth'
p139
NsS'trust_env'
p140
I01
sS'headers'
p141
g1
(crequests.structures
CaseInsensitiveDict
p142
g3
NtRp143
(dp144
S'_store'
p145
curllib3.packages.ordered_dict
OrderedDict
p146
((lp147
(lp148
S'connection'
p149
a(S'Connection'
p150
S'keep-alive'
p151
tp152
aa(lp153
S'accept-encoding'
p154
a(S'Accept-Encoding'
p155
S'gzip, deflate'
tp156
aa(lp157
S'accept'
p158
a(S'Accept'
p159
S'*/*'
p160
tp161
aa(lp162
S'user-agent'
p163
a(S'User-Agent'
p164
S'News Article Downloader /u/JS_Research PRAW/5.2.0 prawcore/0.12.0'
tp165
aatRp166
sbsS'cert'
p167
NsS'params'
p168
(dp169
sS'prefetch'
p170
NsS'verify'
p171
I01
sS'proxies'
p172
(dp173
sS'adapters'
p174
g146
((lp175
(lp176
S'https://'
p177
ag1
(crequests.adapters
HTTPAdapter
p178
g3
NtRp179
(dp180
S'_pool_block'
p181
I00
sS'_pool_maxsize'
p182
I10
sS'max_retries'
p183
g1
(curllib3.util.retry
Retry
p184
g3
NtRp185
(dp186
S'status'
p187
NsS'redirect'
p188
NsS'read'
p189
I00
sS'backoff_factor'
p190
I0
sS'respect_retry_after_header'
p191
I01
sS'history'
p192
(tsS'raise_on_status'
p193
I01
sS'connect'
p194
NsS'status_forcelist'
p195
g48
((ltRp196
sS'total'
p197
I0
sS'raise_on_redirect'
p198
I01
sS'method_whitelist'
p199
c__builtin__
frozenset
p200
((lp201
S'HEAD'
p202
aS'TRACE'
p203
aS'GET'
p204
aS'PUT'
p205
aS'OPTIONS'
p206
aS'DELETE'
p207
atRp208
sbsS'config'
p209
(dp210
sS'_pool_connections'
p211
I10
sbaa(lp212
S'http://'
p213
ag1
(g178
g3
NtRp214
(dp215
g181
I00
sg182
I10
sg183
g1
(g184
g3
NtRp216
(dp217
g187
Nsg188
Nsg189
I00
sg190
I0
sg191
I01
sg192
(tsg193
I01
sg194
Nsg195
g48
((ltRp218
sg197
I0
sg198
I01
sg199
g208
sbsg209
(dp219
sg211
I10
sbaatRp220
sS'max_redirects'
p221
I30
sbsS'reddit_url'
p222
S'https://www.reddit.com'
p223
sS'oauth_url'
p224
S'https://oauth.reddit.com'
p225
sbsbsS'_password'
p226
VXPlstazuMC0CaX1Njhf0ny5u^nMD5t*P73O
p227
sS'_expiration_timestamp'
p228
F1510899012.1773059
sS'refresh_token'
p229
NsbsS'_rate_limiter'
p230
g1
(cprawcore.rate_limit
RateLimiter
p231
g3
NtRp232
(dp233
S'used'
p234
I114
sS'remaining'
p235
F486
sS'next_request_timestamp'
p236
F1510895611.0569813
sS'reset_timestamp'
p237
F1510896000.2545121
sbsbsS'_objector'
p238
g1
(cpraw.objector
Objector
p239
g3
NtRp240
(dp241
g35
g37
sS'parsers'
p242
(dp243
S'LiveUpdate'
p244
cpraw.models.reddit.live
LiveUpdate
p245
sS'ModmailConversation'
p246
cpraw.models.reddit.modmail
ModmailConversation
p247
sS'ModmailMessage'
p248
cpraw.models.reddit.modmail
ModmailMessage
p249
sS't4'
p250
cpraw.models.reddit.message
Message
p251
sS't5'
p252
g20
sS't2'
p253
cpraw.models.reddit.redditor
Redditor
p254
sS't3'
p255
g2
sS't1'
p256
cpraw.models.reddit.comment
Comment
p257
sS'UserList'
p258
cpraw.models.list.redditor
RedditorList
p259
sS'stylesheet'
p260
cpraw.models.stylesheet
Stylesheet
p261
sS'LabeledMulti'
p262
cpraw.models.reddit.multi
Multireddit
p263
sS'Listing'
p264
cpraw.models.listing.listing
Listing
p265
sS'modaction'
p266
cpraw.models.modaction
ModAction
p267
sS'LiveUpdateEvent'
p268
cpraw.models.reddit.live
LiveThread
p269
sS'ModmailAction'
p270
cpraw.models.reddit.modmail
ModmailAction
p271
sS'more'
p272
cpraw.models.reddit.more
MoreComments
p273
ssbsS'subreddits'
p274
g1
(cpraw.models.subreddits
Subreddits
p275
g3
NtRp276
(dp277
g35
g37
sbsg139
g1
(cpraw.models.auth
Auth
p278
g3
NtRp279
(dp280
g35
g37
sbsg19
g1
(cpraw.models.helpers
SubredditHelper
p281
g3
NtRp282
(dp283
g35
g37
sbsS'front'
p284
g1
(cpraw.models.front
Front
p285
g3
NtRp286
(dp287
g35
g37
sS'_comments'
p288
NsS'_path'
p289
S'/'
sbsS'live'
p290
g1
(cpraw.models.helpers
LiveHelper
p291
g3
NtRp292
(dp293
g35
g37
sbsS'inbox'
p294
g1
(cpraw.models.inbox
Inbox
p295
g3
NtRp296
(dp297
g35
g37
sbsS'multireddit'
p298
g1
(cpraw.models.helpers
MultiredditHelper
p299
g3
NtRp300
(dp301
g35
g37
sbsS'_unique_counter'
p302
I0
sg209
g1
(cpraw.config
Config
p303
g3
NtRp304
(dp305
S'username'
p306
g52
sg222
g223
sS'_settings'
p307
(dp308
g306
g52
sg59
g60
sS'password'
p309
g227
sS'user_agent'
p310
VNews Article Downloader /u/JS_Research
p311
sg64
g65
ssS'check_for_updates'
p312
I01
sS'custom'
p313
(dp314
sg61
g63
sg310
g311
sg64
g65
sS'_short_url'
p315
S'https://redd.it'
p316
sg59
g60
sg224
g225
sg309
g227
sS'kinds'
p317
(dp318
g102
g256
sS'message'
p319
g250
sS'redditor'
p320
g253
sS'submission'
p321
g255
sg19
g252
ssg229
g63
sbsS'_read_only_core'
p322
g1
(g40
g3
NtRp323
(dp324
g43
g1
(cprawcore.auth
ReadOnlyAuthorizer
p325
g3
NtRp326
(dp327
g53
Nsg47
Nsg228
Nsg229
Nsg55
g57
sbsg230
g1
(g231
g3
NtRp328
(dp329
g234
Nsg235
Nsg236
Nsg237
NsbsbsS'_authorized_core'
p330
g41
sS'user'
p331
g1
(cpraw.models.user
User
p332
g3
NtRp333
(dp334
g35
g37
sS'_me'
p335
g1
(g254
g3
NtRp336
(dp337
S'is_employee'
p338
I00
sS'has_visited_new_profile'
p339
I00
sS'pref_no_profanity'
p340
I01
sg11
(dp341
sS'is_suspended'
p342
I00
sS'pref_geopopular'
p343
V
sS'_listing_use_sort'
p344
I01
sg19
NsS'is_sponsor'
p345
I00
sS'gold_expiration'
p346
NsS'id'
p347
Ve7dsf19
p348
sS'suspension_expiration_utc'
p349
NsS'verified'
p350
I00
sg27
I00
sS'new_modmail_exists'
p351
NsS'features'
p352
(dp353
Vsearch_public_traffic
p354
(dp355
Vowner
p356
Vsearch
p357
sVvariant
p358
Vnew_search_11
p359
sVexperiment_id
p360
I212
ssVdo_not_track
p361
I01
sVgeopopular_au_holdout
p362
(dp363
Vowner
p364
Vrelevance
p365
sVvariant
p366
Vcontrol_2
p367
sVexperiment_id
p368
I206
ssVshow_amp_link
p369
I01
sVlive_happening_now
p370
I01
sVadserver_reporting
p371
I01
sVgeopopular
p372
I01
sVchat_rollout
p373
I01
sVads_auto_refund
p374
I01
sVlisting_service_rampup
p375
I01
sVmobile_web_targeting
p376
I01
sVdefault_srs_holdout
p377
(dp378
Vowner
p379
Vrelevance
p380
sVvariant
p381
Vtutorial
p382
sVexperiment_id
p383
I171
ssVadzerk_do_not_track
p384
I01
sVusers_listing
p385
I01
sVshow_user_sr_name
p386
I01
sVwhitelisted_pms
p387
I01
sVpersonalization_prefs
p388
I01
sVupgrade_cookies
p389
I01
sVnew_overview
p390
I01
sVnew_report_flow
p391
I01
sVblock_user_by_report
p392
I01
sVadblock_test
p393
I01
sVlegacy_search_pref
p394
I01
sVorangereds_as_emails
p395
I01
sVmweb_xpromo_modal_listing_click_daily_dismissible_ios
p396
I01
sVexpando_events
p397
I01
sVeu_cookie_policy
p398
I01
sVprogrammatic_ads
p399
I01
sVforce_https
p400
I01
sVinbox_push
p401
I01
sVpost_to_profile_beta
p402
I01
sVcrossposting_ga
p403
I01
sVoutbound_clicktracking
p404
I01
sVnew_loggedin_cache_policy
p405
I01
sVshow_secret_santa
p406
I01
sVhttps_redirect
p407
I01
sVsearch_dark_traffic
p408
I01
sVmweb_xpromo_interstitial_comments_ios
p409
I01
sVpause_ads
p410
I01
sVgive_hsts_grants
p411
I01
sVshow_recommended_link
p412
I01
sVmobile_native_banner
p413
I01
sVmweb_xpromo_interstitial_comments_android
p414
I01
sVads_auction
p415
I01
sVgeopopular_se_holdout
p416
(dp417
Vowner
p418
Vrelevance
p419
sVvariant
p420
Vcontrol_2
p421
sVexperiment_id
p422
I224
ssVscreenview_events
p423
I01
sVsubreddit_recommendations_carousel_holdout
p424
(dp425
Vowner
p426
Vrelevance
p427
sVvariant
p428
Vcontrol_2
p429
sVexperiment_id
p430
I239
ssVnew_report_dialog
p431
I01
sVmoat_tracking
p432
I01
sVsubreddit_rules
p433
I01
sVadzerk_reporting_2
p434
I01
sVactivity_service_write
p435
I01
sVads_auto_extend
p436
I01
sVinterest_targeting
p437
I01
sVpost_embed
p438
I01
sVmweb_xpromo_ad_loading_android
p439
(dp440
Vowner
p441
Vchannels
p442
sVvariant
p443
Vcontrol_1
p444
sVexperiment_id
p445
I187
ssVscroll_events
p446
I01
sVmweb_xpromo_modal_listing_click_daily_dismissible_android
p447
I01
sVcrossposting_recent
p448
I01
sVactivity_service_read
p449
I01
ssS'over_18'
p450
I00
sS'is_gold'
p451
I00
sS'is_mod'
p452
I00
sS'has_verified_email'
p453
I00
sS'in_redesign_beta'
p454
I00
sS'has_mod_mail'
p455
I00
sS'oauth_client_id'
p456
VwU1FwN1ugWt7eQ
p457
sS'hide_from_robots'
p458
I00
sS'link_karma'
p459
I1
sg35
g37
sS'inbox_count'
p460
I1
sS'pref_top_karma_subreddits'
p461
NsS'has_mail'
p462
I01
sS'pref_show_snoovatar'
p463
I00
sg104
VJS_Research
p464
sS'created'
p465
F1505744953
sS'_stream'
p466
NsS'gold_creddits'
p467
I0
sS'created_utc'
p468
F1505716153
sS'in_beta'
p469
I00
sS'comment_karma'
p470
I0
sS'has_subscribed'
p471
I00
sg289
S'user/JS_Research/'
p472
sbsbsbsS'_filters'
p473
Nsg466
Nsg288
NsS'_mod'
p474
NsS'_moderator'
p475
Nsg289
S'r/science/'
p476
sbsS'selftext_html'
p477
NsS'selftext'
p478
V
sS'likes'
p479
NsS'suggested_sort'
p480
Vconfidence
p481
sS'user_reports'
p482
(lp483
sS'secure_media'
p484
NsS'is_reddit_media_domain'
p485
I00
sS'link_flair_text'
p486
VMedicine
p487
sg347
V5g3508
p488
sS'banned_at_utc'
p489
NsS'view_count'
p490
NsS'archived'
p491
I01
sS'clicked'
p492
I00
sS'report_reasons'
p493
NsS'title'
p494
VAn algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy, finds Google and Harvard Medical School researchers, based on an evaluation of retinal fundus photographs from adults with diabetes.
p495
sS'num_crossposts'
p496
I0
sS'saved'
p497
I00
sS'mod_reports'
p498
(lp499
sS'can_mod_post'
p500
I00
sS'is_crosspostable'
p501
I00
sS'pinned'
p502
I00
sS'comment_limit'
p503
I2048
sS'score'
p504
I897
sS'approved_by'
p505
Nsg450
I00
sS'hidden'
p506
I00
sS'preview'
p507
(dp508
Vimages
p509
(lp510
(dp511
Vsource
p512
(dp513
Vurl
p514
Vhttps://i.redditmedia.com/zlMk32ZaQaMGBfh1mMun1pAC5BQh8l8D5AwXESxzWNI.jpg?s=c202648e5f1dbc72b1ac3c3aaa827992
p515
sVwidth
p516
I1948
sVheight
p517
I1196
ssVresolutions
p518
(lp519
(dp520
Vurl
p521
Vhttps://i.redditmedia.com/zlMk32ZaQaMGBfh1mMun1pAC5BQh8l8D5AwXESxzWNI.jpg?fit=crop&crop=faces%2Centropy&arh=2&w=108&s=ac799131fffc70100418c33333fbe3db
p522
sVwidth
p523
I108
sVheight
p524
I66
sa(dp525
Vurl
p526
Vhttps://i.redditmedia.com/zlMk32ZaQaMGBfh1mMun1pAC5BQh8l8D5AwXESxzWNI.jpg?fit=crop&crop=faces%2Centropy&arh=2&w=216&s=db8e1be661d34cd96432f21d80a98e77
p527
sVwidth
p528
I216
sVheight
p529
I132
sa(dp530
Vurl
p531
Vhttps://i.redditmedia.com/zlMk32ZaQaMGBfh1mMun1pAC5BQh8l8D5AwXESxzWNI.jpg?fit=crop&crop=faces%2Centropy&arh=2&w=320&s=17d0feb4e67d47df356b71d10a3a2efd
p532
sVwidth
p533
I320
sVheight
p534
I196
sa(dp535
Vurl
p536
Vhttps://i.redditmedia.com/zlMk32ZaQaMGBfh1mMun1pAC5BQh8l8D5AwXESxzWNI.jpg?fit=crop&crop=faces%2Centropy&arh=2&w=640&s=203fee388fcb064eafc8e7937eaacea9
p537
sVwidth
p538
I640
sVheight
p539
I392
sa(dp540
Vurl
p541
Vhttps://i.redditmedia.com/zlMk32ZaQaMGBfh1mMun1pAC5BQh8l8D5AwXESxzWNI.jpg?fit=crop&crop=faces%2Centropy&arh=2&w=960&s=e23f8ce55b916a74cf7f7789021d8887
p542
sVwidth
p543
I960
sVheight
p544
I589
sa(dp545
Vurl
p546
Vhttps://i.redditmedia.com/zlMk32ZaQaMGBfh1mMun1pAC5BQh8l8D5AwXESxzWNI.jpg?fit=crop&crop=faces%2Centropy&arh=2&w=1080&s=776e3d432e2cca16f290e876ad65b600
p547
sVwidth
p548
I1080
sVheight
p549
I663
sasVvariants
p550
(dp551
sVid
p552
VUJC-20hNLMXLyfV9-EL8F0K50SvDL62ybmFnXmrhEcg
p553
sasVenabled
p554
I00
ssS'thumbnail'
p555
Vhttps://a.thumbs.redditmedia.com/CSTrr1LyvCLdTDsTDcFYAthbY-3G--Y_scPyb8IQt08.jpg
p556
sS'subreddit_id'
p557
Vt5_mouw
p558
sS'whitelist_status'
p559
Vall_ads
p560
sS'edited'
p561
I00
sS'link_flair_css_class'
p562
Vmed
p563
sS'author_flair_css_class'
p564
Vmed   reward3
p565
sS'contest_mode'
p566
I00
sS'gilded'
p567
I0
sS'downs'
p568
I0
sS'brand_safe'
p569
I01
sS'secure_media_embed'
p570
(dp571
sS'removal_reason'
p572
NsS'post_hint'
p573
Vlink
p574
sS'stickied'
p575
I00
sg35
g37
sS'can_gild'
p576
I01
sS'thumbnail_height'
p577
I85
sS'parent_whitelist_status'
p578
Vall_ads
p579
sg104
Vt3_5g3508
p580
sg474
NsS'spoiler'
p581
I00
sS'permalink'
p582
V/r/science/comments/5g3508/an_algorithm_based_on_deep_machine_learning_had/
p583
sS'subreddit_type'
p584
Vpublic
p585
sS'locked'
p586
I00
sS'hide_score'
p587
I00
sg465
F1480712649
sS'url'
p588
Vhttp://jamanetwork.com/journals/jama/fullarticle/2588763
p589
sS'author_flair_text'
p590
VMD-PhD-MBA | Clinical Professor/Medicine
p591
sS'quarantine'
p592
I00
sS'author'
p593
g1
(g254
g3
NtRp594
(dp595
g104
Vmvea
p596
sg27
I00
sg11
(dp597
sg344
I01
sg466
Nsg35
g37
sg289
S'user/mvea/'
p598
sbsg468
F1480683849
sS'subreddit_name_prefixed'
p599
Vr/science
p600
sS'ups'
p601
I897
sg34
NsS'media'
p602
NsS'article_text'
p603
VKey Points\u000a\u000aQuestion How does the performance of an automated deep learning algorithm compare with manual grading by ophthalmologists for identifying diabetic retinopathy in retinal fundus photographs?\u000a\u000aFinding In 2 validation sets of 9963 images and 1748 images, at the operating point selected for high specificity, the algorithm had 90.3% and 87.0% sensitivity and 98.1% and 98.5% specificity for detecting referable diabetic retinopathy, defined as moderate or worse diabetic retinopathy or referable macular edema by the majority decision of a panel of at least 7 US board-certified ophthalmologists. At the operating point selected for high sensitivity, the algorithm had 97.5% and 96.1% sensitivity and 93.4% and 93.9% specificity in the 2 validation sets.\u000a\u000aMeaning Deep learning algorithms had high sensitivity and specificity for detecting diabetic retinopathy and macular edema in retinal fundus photographs.\u000a\u000aAbstract\u000a\u000aImportance Deep learning is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior, removing the need to specify rules explicitly. Application of these methods to medical imaging requires further assessment and validation.\u000a\u000aObjective To apply deep learning to create an algorithm for automated detection of diabetic retinopathy and diabetic macular edema in retinal fundus photographs.\u000a\u000aDesign and Setting A specific type of neural network optimized for image classification called a deep convolutional neural network was trained using a retrospective development data set of 128 175 retinal images, which were graded 3 to 7 times for diabetic retinopathy, diabetic macular edema, and image gradability by a panel of 54 US licensed ophthalmologists and ophthalmology senior residents between May and December 2015. The resultant algorithm was validated in January and February 2016 using 2 separate data sets, both graded by at least 7 US board-certified ophthalmologists with high intragrader consistency.\u000a\u000aExposure Deep learningtrained algorithm.\u000a\u000aMain Outcomes and Measures The sensitivity and specificity of the algorithm for detecting referable diabetic retinopathy (RDR), defined as moderate and worse diabetic retinopathy, referable diabetic macular edema, or both, were generated based on the reference standard of the majority decision of the ophthalmologist panel. The algorithm was evaluated at 2 operating points selected from the development set, one selected for high specificity and another for high sensitivity.\u000a\u000aResults The EyePACS-1 data set consisted of 9963 images from 4997 patients (mean age, 54.4 years; 62.2% women; prevalence of RDR, 683/8878 fully gradable images [7.8%]); the Messidor-2 data set had 1748 images from 874 patients (mean age, 57.6 years; 42.6% women; prevalence of RDR, 254/1745 fully gradable images [14.6%]). For detecting RDR, the algorithm had an area under the receiver operating curve of 0.991 (95% CI, 0.988-0.993) for EyePACS-1 and 0.990 (95% CI, 0.986-0.995) for Messidor-2. Using the first operating cut point with high specificity, for EyePACS-1, the sensitivity was 90.3% (95% CI, 87.5%-92.7%) and the specificity was 98.1% (95% CI, 97.8%-98.5%). For Messidor-2, the sensitivity was 87.0% (95% CI, 81.1%-91.0%) and the specificity was 98.5% (95% CI, 97.7%-99.1%). Using a second operating point with high sensitivity in the development set, for EyePACS-1 the sensitivity was 97.5% and specificity was 93.4% and for Messidor-2 the sensitivity was 96.1% and specificity was 93.9%.\u000a\u000aConclusions and Relevance In this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment.\u000a\u000aIntroduction\u000a\u000aAmong individuals with diabetes, the prevalence of diabetic retinopathy is approximately 28.5% in the United States1 and 18% in India.2 Most guidelines recommend annual screening for those with no retinopathy or mild diabetic retinopathy, repeat examination in 6 months for moderate diabetic retinopathy, and an ophthalmologist referral for treatment evaluation within a few weeks to months for severe or worse diabetic retinopathy or the presence of referable diabetic macular edema, known as clinically significant macular edema.3 Referable diabetic retinopathy has been defined as moderate or worse diabetic retinopathy or referable diabetic macular edema,4 given that recommended management changes from yearly screening to closer follow-up at moderate disease severity.3 Retinal photography with manual interpretation is a widely accepted screening tool for diabetic retinopathy, with performance that can exceed that of in-person dilated eye examinations.3\u000a\u000aAutomated grading of diabetic retinopathy has potential benefits such as increasing efficiency, reproducibility, and coverage of screening programs; reducing barriers to access; and improving patient outcomes by providing early detection and treatment. To maximize the clinical utility of automated grading, an algorithm to detect referable diabetic retinopathy is needed. Machine learning (a discipline within computer science that focuses on teaching machines to detect patterns in data) has been leveraged for a variety of classification tasks including automated classification of diabetic retinopathy. However, much of the work has focused on feature-engineering, which involves computing explicit features specified by experts, resulting in algorithms designed to detect specific lesions or predicting the presence of any level of diabetic retinopathy.5 Deep learning6 is a machine learning technique that avoids such engineering by learning the most predictive features directly from the images given a large data set of labeled examples. This technique uses an optimization algorithm called back-propagation to indicate how a machine should change its internal parameters to best predict the desired output of an image.\u000a\u000aIn this study, deep learning7,8 was used to train an algorithm to detect referable diabetic retinopathy and assess the performance of the algorithm in 2 clinical validation sets.\u000a\u000aMethods\u000a\u000aData Sets\u000a\u000aFor algorithm development, macula-centered retinal fundus images were retrospectively obtained from EyePACS in the United States and 3 eye hospitals in India (Aravind Eye Hospital, Sankara Nethralaya, and Narayana Nethralaya) among patients presenting for diabetic retinopathy screening. All images were deidentified according to Health Insurance Portability and Accountability Act Safe Harbor prior to transfer to study investigators. Ethics review and institutional review board exemption was obtained using Quorum Review IRB.\u000a\u000aTwo further data sets were used for clinical validation. The first deidentified data set consisted of a random sample of macula-centered images taken at EyePACS screening sites between May 2015 and October 2015. A variety of cameras were used, including Centervue DRS, Optovue iCam, Canon CR1/DGi/CR2, and Topcon NW using 45 fields of view. EyePACS images were acquired as a part of routine clinical care for diabetic retinopathy screening, and approximately 40% of the images were acquired with pupil dilation. This data set did not overlap with the EyePACS data used in development. The second data set was the publicly available Messidor-2 data set,9,10 which has been used by other groups for benchmarking performance of automated detection algorithms for diabetic retinopathy.11- 13 The images were obtained between January 2005 and December 2010 at 3 hospitals in France using a Topcon TRC NW6 nonmydriatic camera and 45 fields of view centered on the fovea. Approximately 44% of images were acquired with pupil dilation.\u000a\u000aGrading\u000a\u000aAll images in the development and clinical validation sets were graded by ophthalmologists for the presence of diabetic retinopathy, diabetic macular edema, and image quality using an annotation tool (eFigures 1 and 2 in the Supplement). Diabetic retinopathy severity (none, mild, moderate, severe, or proliferative) was graded according to the International Clinical Diabetic Retinopathy scale.14 Referable diabetic macular edema was defined as any hard exudates within 1 disc diameter of the macula,15 which is a proxy for macular edema when stereoscopic views are not available. Image quality was assessed by graders using the rubric in the Grading Instructions section in the Supplement. Images of excellent, good, and adequate quality were considered gradable.\u000a\u000aDerivation\u000a\u000aThe 54 graders for the development set were US-licensed ophthalmologists or ophthalmology trainees in their last year of residency (postgraduate year 4). Each individual graded between 20 and 62 508 images (mean, 9774; median, 2021). Trainee performance was not worse than that of licensed ophthalmologists, although only 3 trainees graded more than 1000 images. Intergrader reliability was measured for each physician using pairwise comparisons by taking the number of times a grader was in agreement with another grader over the total number of pairwise comparisons. Approximately 10% of the derivation set (128 175 images) were randomly selected to be overread by the same grader to determine intragrader reliability. All graders were paid for their work.\u000a\u000aValidation\u000a\u000aGraders who were US board-certified ophthalmologists with the highest rate of self-consistency were invited to grade the clinical validation sets, EyePACS-1 (n = 8) and Messidor-2 (n = 7). A simple majority decision (an image was classified as referable if 50% of ophthalmologists graded it referable) served as the reference standard for both referability and gradability. Graders were masked to judgments by other graders. (See Grading Quality Control section in the Supplement for more details.)\u000a\u000aDevelopment of the Algorithm\u000a\u000aDeep learning is the process of training a neural network (a large mathematical function with millions of parameters) to perform a given task. The function computes diabetic retinopathy severity from the intensities of the pixels in a fundus image. Creating or training this function requires a large set of images for which the diabetic retinopathy severity is already known (training set). During the training process, the parameters of the neural network (mathematical function) are initially set to random values. Then, for each image, the severity grade given by the function is compared with the known grade from the training set, and parameters of the function are then modified slightly to decrease the error on that image. This process is repeated for every image in the training set many times over, and the function learns how to accurately compute the diabetic retinopathy severity from the pixel intensities of the image for all images in the training set. With the right training data, the result is a function general enough to compute diabetic retinopathy severity on new images. The network used in this study is a convolutional neural network that uses a function that first combines nearby pixels into local features, then aggregates those into global features. Although the algorithm does not explicitly detect lesions (eg, hemorrhages, microaneurysms), it likely learns to recognize them using the local features. The specific neural network used in this work is the Inception-v3 architecture proposed by Szegedy et al.8\u000a\u000aData were preprocessed according to a protocol described in the Supplement. The optimization algorithm used to train the network weights was a distributed stochastic gradient descent implementation by Dean et al.16 To speed up the training, batch normalization7 as well as preinitialization using weights from the same network trained to classify objects in the ImageNet data set17 were used. Preinitialization also improved performance. A single network was trained to make multiple binary predictions, including whether the image was (1) moderate or worse diabetic retinopathy (ie, moderate, severe, or proliferative), (2) severe or worse diabetic retinopathy, (3) referable diabetic macular edema, or (4) fully gradable. Referable diabetic retinopathy was defined as any image that fulfilled either criterion 1, criterion 3, or both.\u000a\u000aThe performance of the algorithm was measured by the area under the receiver operating curve (AUC) generated by plotting sensitivity vs 1  specificity. Because the network in this study had a large number of parameters (22 million), an early stopping criteria18 (that stops training when peak AUC is reached on a separate tuning set) was used to terminate training before convergence. The development set was divided into 2 parts: (1) training: 80% of the data was used to optimize the network weights and (2) tuning: 20% of the data was used to optimize hyperparameters (such as early stopping for training, image preprocessing options). An ensemble19 of 10 networks trained on the same data was used, and the final prediction was computed by a linear average over the predictions of the ensemble.\u000a\u000aEvaluating the Algorithm\u000a\u000aThe trained neural network generates a continuous number between 0 and 1 for referable diabetic retinopathy and other diabetic retinopathy classifications, corresponding to the probability of that condition being present in the image. Receiver operating curves were plotted by varying the operating threshold and 2 operating points for the algorithm were selected from the development set. The first operating point approximated the specificity of the ophthalmologists in the derivation set for detecting referable diabetic retinopathy (approximately 98%) and allowed for better comparison between the algorithms performance and that of the 7 or 8 ophthalmologists that graded the validation set. The second operating point corresponded to a sensitivity of 97% for detecting referable diabetic retinopathy because a high sensitivity is a prerequisite in a potential screening tool.\u000a\u000aStatistical Analysis and Performance Comparison on Clinical Validation Sets\u000a\u000aBased on the 2 operating points, 2  2 tables were generated to characterize the sensitivity and specificity of the algorithm with respect to the reference standard, which was defined as the majority decision of the ophthalmologists readings based on all available grades. The 95% confidence intervals for the sensitivity and specificity of the algorithm at the 2 operating points were calculated to be exact Clopper-Pearson intervals,20 which corresponded to separate 2-sided confidence intervals with individual coverage probabilities of sqrt(0.95)  0.975. The 95% confidence intervals for the intragrader and intergrader reliabilities are z confidence intervals.\u000a\u000aStatistical significance and simultaneous 2-sided confidence intervals were computed using the StatsModels version 0.6.1 and SciPy version 0.15.1 python packages.\u000a\u000aSubsampling Experiments\u000a\u000aExperiments to understand the relationship between the amount of development data on the performance of the resulting algorithms also were conducted. To understand the effect of reducing the number of images in the training set, images were sampled at rates of 0.2%, 2%, and N  10%, in which N ranged from 1 to 10; a new algorithm was trained for each data set; and its performance was measured on a fixed tuning set. To understand the effect of reducing the number of grades per image, 2 experiments were run: (1) training: grades in the training set were subsampled at rates of N  20%, in which N ranged from 0 to 5, with a restriction that the minimum number of grades to sample per image was 1. A new algorithm was trained for each N, and its performance was measured on a fixed tuning set with all the available grades and (2) tuning: grades in the tuning set (used to measure performance) were sampled using the same procedure as in the training experiment. The training set and the algorithm were fixed and used all available grades.\u000a\u000aResults\u000a\u000aPatient demographics and image characteristics are summarized in the Table. The development set included 128 175 images, of which 118 419 were assessed for referable diabetic retinopathy and 33 246 (28.1%) had referable diabetic retinopathy. Each image was graded by ophthalmologists between 3 and 7 times. The EyePACS-1 and Messidor-2 clinical validation sets consisted of 9963 images (8788 fully gradable; 683 [7.8%] referable) and 1748 images (1745 fully gradable; 254 [14.6%] referable), respectively. Image quality was assessed only for a subset of the development set, and fully gradable images ranged from 75.1% (52 311 of 69 598 images that were assessed for image quality) for the development set to 99.8% (1745 of 1748) for the Messidor-2 validation set (Table and Figure 1).\u000a\u000aIn the development set, intragrader reliability among the ophthalmologists could be assessed among 16 graders who had graded a sufficient volume of repeat images. The mean intragrader reliability for referable diabetic retinopathy of these graders was 94.0% (95% CI, 91.2%-96.8%). Intergrader reliability could be assessed on 26 graders. The mean intergrader reliability for these graders was 95.5% (95% CI, 94.0%-96.9%).\u000a\u000aIn the validation sets, a total of 8 grades per image were obtained for the EyePACS-1 data set and 7 grades per image for Messidor-2. The mean intragrader reliability for referable diabetic retinopathy for EyePACS-1 was 95.8% (95% CI, 92.8%-98.7%). Intragrader reliability was not assessed for Messidor-2. The mean intergrader reliability was 95.9% (95% CI, 94.0%-97.8%) for EyePACS-1 and 94.6% (95% CI, 93.0%-96.1%) for Messidor-2.\u000a\u000aOn EyePACS-1, the mean agreement among ophthalmologists on referable diabetic retinopathy images was 77.7% (SD, 16.3%), with complete agreement on 19.6% of the referable cases. On nonreferable images, the average agreement was 97.4% (SD, 7.3%), with complete agreement on 85.6% of the nonreferable cases. On Messidor-2, the average agreement among ophthalmologists on referable diabetic retinopathy images was 82.4% (SD, 16.9%), with complete agreement on 37.8% of the referable cases. On nonreferable images, the average agreement was 96.3% (SD, 9.9%), with complete agreement on 85.1% of the nonreferable cases. The distribution of agreement among ophthalmologists is reported in eFigure 3 in the Supplement.\u000a\u000aFigure 2 summarizes the performance of the algorithm in detecting referable diabetic retinopathy in the EyePACS-1 and Messidor-2 validation data sets for fully gradable images. For referable diabetic retinopathy, the algorithm achieved an AUC of 0.991 (95% CI, 0.988-0.993) on EyePACS-1 and an AUC of 0.990 (95% CI, 0.986-0.995) on Messidor-2. Using the first operating cut point with high specificity, approximating the specificity of ophthalmologists in the development set, on EyePACS-1, the algorithms sensitivity was 90.3% and specificity was 98.1%. In Messidor-2, the sensitivity was 87.0% and specificity was 98.5%.\u000a\u000aA second operating point for the algorithm was evaluated, which had a high sensitivity on the development set, reflecting an output that would be used for a screening tool. Using this operating point, on EyePACS-1, the algorithm had a sensitivity of 97.5% (95% CI, 95.8%-98.7%) and a specificity of 93.4% (95% CI, 92.8%-94.0%). In Messidor-2, the sensitivity was 96.1% (95% CI, 92.4%-98.3%) and the specificity was 93.9% (95% CI, 92.4%-95.3%). Given an approximately 8% prevalence of referable diabetic retinopathy (on a per-image basis [Table]), these findings correspond to a negative predictive value of 99.8% for EyePACS-1 and 99.6% for Messidor-2.\u000a\u000aThe algorithm performance in making all-cause referable predictions, defined as moderate or worse diabetic retinopathy, referable diabetic macular edema, or ungradable images (Figure 3), was also evaluated using the EyePACS-1 data set. The Messidor-2 data set had only 3 ungradable images, so it was omitted from this analysis. For this task, the algorithm achieved an AUC of 0.974 (95% CI, 0.971-0.978). At the first (high-specificity) operating point, the algorithm had a sensitivity of 90.7% (95% CI, 89.2%-92.1%) and a specificity of 93.8% (95% CI, 93.2%-94.4%). At the second (high-sensitivity) operating point, the algorithm had a sensitivity of 96.7% (95% CI, 95.7%-97.5%) and a specificity of 84.0% (95% CI, 83.1%-85.0%).\u000a\u000aAdditional sensitivity analyses were conducted for several subcategories: (1) detecting moderate or worse diabetic retinopathy only; (2) detecting severe or worse diabetic retinopathy only; (3) detecting referable diabetic macular edema only; (4) image quality; and (5) referable diabetic retinopathy on 2 data sets, each restricted to mydriatic and nonmydriatic images, respectively. For each subcategory 1 through 4, the algorithm achieved high sensitivity and specificity (see section on Performance on Individual Diabetic Retinopathy Subtypes, Image Quality, eTable 1, and eFigure 4 in the Supplement). For example, for the EyePACS-1 data set, at the first operating point for moderate or worse diabetic retinopathy, the algorithm had a sensitivity of 90.1% (95% CI, 87.2%-92.6%) and specificity of 98.2% (95% CI, 97.8%-98.5%). For severe or worse diabetic retinopathy only at the first operating point, the algorithm had a sensitivity of 84.0% (95% CI, 75.3%-90.6%) and specificity of 98.8% (95% CI, 98.5%-99.0%). For diabetic macular edema only, the algorithms sensitivity was 90.8% (95% CI, 86.1%-94.3%) and specificity was 98.7% (95% CI, 98.4%-99.0%). The algorithms performance on mydriatic images was very close to its performance on nonmydriatic images (and both were similar to the overall algorithm performance; see eTable 2 in the Supplement).\u000a\u000aMultiple networks with varying number of images and grades per image were trained to determine how smaller training data sets related to the performance of the trained algorithms. In the first subsampling experiment (Figure 4A), the effects of data set size on algorithm performance were examined and shown to plateau at around 60 000 images (or approximately 17 000 referable images). In the second experiment (Figure 4B) on subsampling grades, 2 trends emerged: (1) increasing the number of grades per image on the training set did not yield an increase in relative performance (31.6% absolute difference) and (2) using only 1 grade per image on the tuning set led to a decline of 36% in performance compared with using all the available grades on the tuning set (an average of 4.5 grades), and that performance steadily increased as more grades were made available for the tuning set. This suggests that additional grading resources should be devoted to grading the tuning set (on which evaluation is done), which improves the quality of the reference standard and the algorithm performance.\u000a\u000aDiscussion\u000a\u000aThese results demonstrate that deep neural networks can be trained, using large data sets and without having to specify lesion-based features, to identify diabetic retinopathy or diabetic macular edema in retinal fundus images with high sensitivity and high specificity. This automated system for the detection of diabetic retinopathy offers several advantages, including consistency of interpretation (because a machine will make the same prediction on a specific image every time), high sensitivity and specificity, and near instantaneous reporting of results. In addition, because an algorithm can have multiple operating points, its sensitivity and specificity can be tuned to match requirements for specific clinical settings, such as high sensitivity for a screening setting. In this study, sensitivities of 97.5% and 96.1% were achieved.\u000a\u000aAutomated and semiautomated diabetic retinopathy evaluation has been previously studied by other groups. Abrmoff et al4 reported a sensitivity of 96.8% at a specificity of 59.4% for detecting referable diabetic retinopathy on the publicly available Messidor-2 data set.9 Solanki et al12 reported a sensitivity of 93.8% at a specificity of 72.2% on the same data set. A study by Philip et al21 reported a sensitivity of 86.2% at a specificity of 76.8% for predicting disease vs no disease on their own data set of 14 406 images. In a recent Kaggle machine-learning competition,22 deep learning was used to predict the diabetic retinopathy grade only (no diabetic macular edema prediction). The winning entry had a performance that was comparable with an ophthalmologist grading the wrong eye of the same patient23 and higher than the agreement demonstrated between general physicians and ophthalmologists.24 Although there are differences in the data set and reference standards compared with the previous studies, the present study extends this body of work by using deep convolutional neural networks and a large data set with multiple grades per image to generate an algorithm with 97.5% sensitivity and 93.4% specificity (at the screening operating point, which had been selected to have high sensitivity). When screening populations with substantial disease, achieving both high sensitivity and high specificity is critical to minimize both false-positive and false-negative results.\u000a\u000aIn the future, based on the observations from this study, the development of similar high-performing algorithms for medical imaging using deep learning has 2 prerequisites. First, there must be collection of a large developmental set with tens of thousands of abnormal cases. While performance on the tuning set saturated at 60 000 images, additional gains might be achieved by increasing the diversity of training data (ie, data from new clinics). Second, data sets used to measure final performance (tuning and the clinical validation data sets) should have multiple grades per image. This provides a more reliable measure of a models final predictive ability. Although intergrader variability is a well-known issue in many settings in which human interpretation is used as the reference standard (as opposed to hard outcomes like mortality), such as in radiology25 or pathology,26 diseases with unambiguous interpretations may not require additional grades per image.\u000a\u000aThere are limitations to this system. The reference standard used for this study was the majority decision of all ophthalmologist graders. This means the algorithm may not perform as well for images with subtle findings that a majority of ophthalmologists would not identify. Another fundamental limitation arises from the nature of deep networks, in which the neural network was provided with only the image and associated grade, without explicit definitions of features (eg, microaneurysms, exudates). Because the network learned the features that were most predictive for the referability implicitly, it is possible that the algorithm is using features previously unknown to or ignored by humans. Although this study used images from a variety of clinical settings (hundreds of clinical sites: 3 in India, hundreds in the United States, and 3 in France) with a range of camera types to mitigate the risk that the algorithm is using anomalies in data acquisition to make predictions, the exact features being used are still unknown. Understanding what a deep neural net uses to make predictions is a very active area of research within the larger machine learning community. Another open question is whether the design of the user interface and the online setting for grading used by ophthalmologists has any influence on their performance relative to a clinical setting. This needs further experiments to address. The algorithm has been trained to identify only diabetic retinopathy and diabetic macular edema. It may miss nondiabetic retinopathy lesions that it was not trained to identify. Hence, this algorithm is not a replacement for a comprehensive eye examination, which has many components, such as visual acuity, refraction, slitlamp examination, and eye pressure measurements. In addition, further validation of the algorithm is necessary in a data set in which the gold standard was not a consensus of experts who participated in the derivation of the algorithm.\u000a\u000aConclusions\u000a\u000aIn this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment.\u000a\u000aBack to top Article Information\u000a\u000aCorresponding Author: Lily Peng, MD, PhD, Google Research, 1600 Amphitheatre Way, Mountain View, CA 94043 (lhpeng@google.com).\u000a\u000aCorrection: This article was corrected on December 13, 2016, for a numerical error in Figure 1.\u000a\u000aPublished Online: November 29, 2016. doi:10.1001/jama.2016.17216\u000a\u000aAuthor Contributions: Drs Gulshan and Peng had full access to all of the data in the study and take responsibility for the integrity of the data and the accuracy of the data analysis. Drs Gulshan and Peng contributed equally to the study.\u000a\u000aConcept and design: Gulshan, Peng, Stumpe, Wu, Venugopalan, Kim, Raman, Nelson, Webster.\u000a\u000aAcquisition, analysis, or interpretation of data: Gulshan, Peng, Coram, Stumpe, Wu, Narayanaswamy, Widner, Madams, Cuadros, Kim, Nelson, Mega, Webster.\u000a\u000aDrafting of the manuscript: Gulshan, Peng, Narayanaswamy, Venugopalan, Madams, Webster.\u000a\u000aCritical revision of the manuscript for important intellectual content: Gulshan, Peng, Coram, Stumpe, Wu, Narayanaswamy, Widner, Cuadros, Kim, Raman, Nelson, Mega, Webster.\u000a\u000aStatistical analysis: Gulshan, Peng, Coram, Stumpe, Wu, Narayanaswamy, Venugopalan, Webster.\u000a\u000aObtained funding: Nelson.\u000a\u000aAdministrative, technical, or material support: Gulshan, Peng, Widner, Madams, Cuadros, Kim, Raman, Nelson, Mega, Webster.\u000a\u000aStudy supervision: Peng, Stumpe, Nelson, Webster.\u000a\u000aConflict of Interest Disclosures: All authors have completed and submitted the ICMJE Form for Disclosure of Potential Conflicts of Interest. Drs Peng, Gulshan, Coram, Stumpe, and Narayanaswamy and Messers Wu and Nelson report a patent pending on processing fundus images using machine learning models. Dr Cuadros reports receipt of grants from Google Inc and the California Health Care Foundation for preparation of data analysis. No other disclosures were reported.\u000a\u000aFunding/Support:Google Inc sponsored the study. Aravind, Sankara, and EyePACS received funding from Google to support extraction, deidentification, and transfer of images for the study (Dr Kim is affiliated with Aravind, Dr Raman is affiliated with Sankara, and Dr Cuadros is affiliated with EyePACS).\u000a\u000aRole of the Funder/Sponsor: Google Inc was involved in the design and conduct of the study; collection, management, analysis, and interpretation of the data; and preparation, review, or approval of the manuscript; and decision to submit the manuscript for publication.\u000a\u000aAdditional Contributions: For technical advice and discussion, we thank the following, who are all employees of Google Inc: Jeff Dean, PhD, Greg Corrado, PhD, George Dahl, PhD, Julian Ibarz, MS, Alexander Toshev, PhD, Patrick Riley, PhD, Eric Christiansen, MS, Mike Sparandara, MS, and Nathan Silberman, PhD. For data acquisition and collection, clinical interpretation, and discussions, we thank Prem Ramaswami, MBA, and Monisha Varadan, MBA (both from Google Inc). The Messidor-2 data set was provided by the Messidor program partners (http://www.adcis.net/en/Download-Third-Party/Messidor.html) and the LaTIM laboratory (http://latim.univ-brest.fr/). No financial compensation was received outside of the contributors regular salaries.
p604
sS'num_comments'
p605
I29
sS'is_self'
p606
I00
sS'visited'
p607
I00
sS'num_reports'
p608
NsS'is_video'
p609
I00
sS'distinguished'
p610
Nsg27
I00
sb.