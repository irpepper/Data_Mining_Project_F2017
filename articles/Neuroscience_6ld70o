ccopy_reg
_reconstructor
p1
(cpraw.models.reddit.submission
Submission
p2
c__builtin__
object
p3
NtRp4
(dp5
S'domain'
p6
Vjournal.frontiersin.org
p7
sS'approved_at_utc'
p8
NsS'_comments_by_id'
p9
(dp10
sS'_info_params'
p11
(dp12
sS'banned_by'
p13
NsS'comment_sort'
p14
S'best'
p15
sS'media_embed'
p16
(dp17
sS'thumbnail_width'
p18
I140
sS'subreddit'
p19
g1
(cpraw.models.reddit.subreddit
Subreddit
p20
g3
NtRp21
(dp22
S'_wiki'
p23
NsS'display_name'
p24
Vscience
p25
sS'_stylesheet'
p26
NsS'_fetched'
p27
I00
sg11
(dp28
sS'_contributor'
p29
NsS'_banned'
p30
NsS'_muted'
p31
NsS'_quarantine'
p32
NsS'_modmail'
p33
NsS'_flair'
p34
NsS'_reddit'
p35
g1
(cpraw.reddit
Reddit
p36
g3
NtRp37
(dp38
S'_core'
p39
g1
(cprawcore.sessions
Session
p40
g3
NtRp41
(dp42
S'_authorizer'
p43
g1
(cprawcore.auth
ScriptAuthorizer
p44
g3
NtRp45
(dp46
S'scopes'
p47
c__builtin__
set
p48
((lp49
V*
atRp50
sS'_username'
p51
VJS_Research
p52
sS'access_token'
p53
VoR7ueivAi-yeTmPh7RRrUoU-sm8
p54
sS'_authenticator'
p55
g1
(cprawcore.auth
TrustedAuthenticator
p56
g3
NtRp57
(dp58
S'client_secret'
p59
VBgciYaBpyH8ef21jXMyJjn_DARE
p60
sS'redirect_uri'
p61
g1
(cpraw.config
_NotSet
p62
g3
NtRp63
sS'client_id'
p64
VwU1FwN1ugWt7eQ
p65
sS'_requestor'
p66
g1
(cprawcore.requestor
Requestor
p67
g3
NtRp68
(dp69
S'_http'
p70
g1
(crequests.sessions
Session
p71
g3
NtRp72
(dp73
S'cookies'
p74
g1
(crequests.cookies
RequestsCookieJar
p75
g3
NtRp76
(dp77
S'_now'
p78
I1510895610
sS'_policy'
p79
(icookielib
DefaultCookiePolicy
p80
(dp81
S'strict_rfc2965_unverifiable'
p82
I01
sS'strict_ns_domain'
p83
I0
sS'_allowed_domains'
p84
NsS'rfc2109_as_netscape'
p85
NsS'rfc2965'
p86
I00
sS'strict_domain'
p87
I00
sg78
I1510895610
sS'strict_ns_set_path'
p88
I00
sS'strict_ns_unverifiable'
p89
I00
sS'strict_ns_set_initial_dollar'
p90
I00
sS'hide_cookie2'
p91
I00
sS'_blocked_domains'
p92
(tsS'netscape'
p93
I01
sbsS'_cookies'
p94
(dp95
S'.reddit.com'
p96
(dp97
S'/'
(dp98
S'loid'
p99
(icookielib
Cookie
p100
(dp101
S'comment'
p102
Nsg6
S'.reddit.com'
p103
sS'name'
p104
g99
sS'domain_initial_dot'
p105
I00
sS'expires'
p106
I1573967423
sS'value'
p107
S'00000000000e7dsf19.2.1505716153623.Z0FBQUFBQmFEbTlBU1EwbzUyZ293WTJBdDJaamk2V2s2dzRJQmlEV3MxUTQxN212M2ZKZjlhN0xrRnVlSUZoODZzUHZqeEdpdzZ1aFAyT0ZVdUQtRm85dzNpcDNfeG0wbi1iZGs3OWF6WXg4RzNqejI0SV9lUllITXBkUGF4LU14ejUzMWxtLTBHT2Q'
p108
sS'domain_specified'
p109
I01
sS'_rest'
p110
(dp111
sS'version'
p112
I0
sS'port_specified'
p113
I00
sS'rfc2109'
p114
I00
sS'discard'
p115
I00
sS'path_specified'
p116
I01
sS'path'
p117
S'/'
sS'port'
p118
NsS'comment_url'
p119
NsS'secure'
p120
I01
sbsS'session_tracker'
p121
(icookielib
Cookie
p122
(dp123
g102
Nsg6
S'.reddit.com'
p124
sg104
S'session_tracker'
p125
sg105
I00
sg106
I1510902809
sg107
S'crQiBtrEVfCCgtFogt.0.1510895610205.Z0FBQUFBQmFEbV82eHgzMEcwQ0hsRTIwSDVVRTJYcUJCVHJVbkh2a0V2ZnlkWS04a2ZnVkExdW5JQmtxdHN2Z005LXZ1RzZQVGNlSGVPU1JQNmwyMkdxdG9LWGlwNVdKRlQxT0JmTlBxUzludEQtblBxeWViNWNUeEZMSzAxWkVXT1E4TkY3bDl4dDU'
p126
sg109
I01
sg110
(dp127
sg112
I0
sg113
I00
sg114
I00
sg115
I00
sg116
I01
sg117
S'/'
sg118
Nsg119
Nsg120
I01
sbsS'edgebucket'
p128
(icookielib
Cookie
p129
(dp130
g102
Nsg6
S'.reddit.com'
p131
sg104
g128
sg105
I00
sg106
I1573967422
sg107
S'wR7VzApWCkJPkoMBgW'
p132
sg109
I01
sg110
(dp133
sg112
I0
sg113
I00
sg114
I00
sg115
I00
sg116
I01
sg117
S'/'
sg118
Nsg119
Nsg120
I01
sbssssbsS'stream'
p134
I00
sS'hooks'
p135
(dp136
S'response'
p137
(lp138
ssS'auth'
p139
NsS'trust_env'
p140
I01
sS'headers'
p141
g1
(crequests.structures
CaseInsensitiveDict
p142
g3
NtRp143
(dp144
S'_store'
p145
curllib3.packages.ordered_dict
OrderedDict
p146
((lp147
(lp148
S'connection'
p149
a(S'Connection'
p150
S'keep-alive'
p151
tp152
aa(lp153
S'accept-encoding'
p154
a(S'Accept-Encoding'
p155
S'gzip, deflate'
tp156
aa(lp157
S'accept'
p158
a(S'Accept'
p159
S'*/*'
p160
tp161
aa(lp162
S'user-agent'
p163
a(S'User-Agent'
p164
S'News Article Downloader /u/JS_Research PRAW/5.2.0 prawcore/0.12.0'
tp165
aatRp166
sbsS'cert'
p167
NsS'params'
p168
(dp169
sS'prefetch'
p170
NsS'verify'
p171
I01
sS'proxies'
p172
(dp173
sS'adapters'
p174
g146
((lp175
(lp176
S'https://'
p177
ag1
(crequests.adapters
HTTPAdapter
p178
g3
NtRp179
(dp180
S'_pool_block'
p181
I00
sS'_pool_maxsize'
p182
I10
sS'max_retries'
p183
g1
(curllib3.util.retry
Retry
p184
g3
NtRp185
(dp186
S'status'
p187
NsS'redirect'
p188
NsS'read'
p189
I00
sS'backoff_factor'
p190
I0
sS'respect_retry_after_header'
p191
I01
sS'history'
p192
(tsS'raise_on_status'
p193
I01
sS'connect'
p194
NsS'status_forcelist'
p195
g48
((ltRp196
sS'total'
p197
I0
sS'raise_on_redirect'
p198
I01
sS'method_whitelist'
p199
c__builtin__
frozenset
p200
((lp201
S'HEAD'
p202
aS'TRACE'
p203
aS'GET'
p204
aS'PUT'
p205
aS'OPTIONS'
p206
aS'DELETE'
p207
atRp208
sbsS'config'
p209
(dp210
sS'_pool_connections'
p211
I10
sbaa(lp212
S'http://'
p213
ag1
(g178
g3
NtRp214
(dp215
g181
I00
sg182
I10
sg183
g1
(g184
g3
NtRp216
(dp217
g187
Nsg188
Nsg189
I00
sg190
I0
sg191
I01
sg192
(tsg193
I01
sg194
Nsg195
g48
((ltRp218
sg197
I0
sg198
I01
sg199
g208
sbsg209
(dp219
sg211
I10
sbaatRp220
sS'max_redirects'
p221
I30
sbsS'reddit_url'
p222
S'https://www.reddit.com'
p223
sS'oauth_url'
p224
S'https://oauth.reddit.com'
p225
sbsbsS'_password'
p226
VXPlstazuMC0CaX1Njhf0ny5u^nMD5t*P73O
p227
sS'_expiration_timestamp'
p228
F1510899012.1773059
sS'refresh_token'
p229
NsbsS'_rate_limiter'
p230
g1
(cprawcore.rate_limit
RateLimiter
p231
g3
NtRp232
(dp233
S'used'
p234
I114
sS'remaining'
p235
F486
sS'next_request_timestamp'
p236
F1510895611.0569813
sS'reset_timestamp'
p237
F1510896000.2545121
sbsbsS'_objector'
p238
g1
(cpraw.objector
Objector
p239
g3
NtRp240
(dp241
g35
g37
sS'parsers'
p242
(dp243
S'LiveUpdate'
p244
cpraw.models.reddit.live
LiveUpdate
p245
sS'ModmailConversation'
p246
cpraw.models.reddit.modmail
ModmailConversation
p247
sS'ModmailMessage'
p248
cpraw.models.reddit.modmail
ModmailMessage
p249
sS't4'
p250
cpraw.models.reddit.message
Message
p251
sS't5'
p252
g20
sS't2'
p253
cpraw.models.reddit.redditor
Redditor
p254
sS't3'
p255
g2
sS't1'
p256
cpraw.models.reddit.comment
Comment
p257
sS'UserList'
p258
cpraw.models.list.redditor
RedditorList
p259
sS'stylesheet'
p260
cpraw.models.stylesheet
Stylesheet
p261
sS'LabeledMulti'
p262
cpraw.models.reddit.multi
Multireddit
p263
sS'Listing'
p264
cpraw.models.listing.listing
Listing
p265
sS'modaction'
p266
cpraw.models.modaction
ModAction
p267
sS'LiveUpdateEvent'
p268
cpraw.models.reddit.live
LiveThread
p269
sS'ModmailAction'
p270
cpraw.models.reddit.modmail
ModmailAction
p271
sS'more'
p272
cpraw.models.reddit.more
MoreComments
p273
ssbsS'subreddits'
p274
g1
(cpraw.models.subreddits
Subreddits
p275
g3
NtRp276
(dp277
g35
g37
sbsg139
g1
(cpraw.models.auth
Auth
p278
g3
NtRp279
(dp280
g35
g37
sbsg19
g1
(cpraw.models.helpers
SubredditHelper
p281
g3
NtRp282
(dp283
g35
g37
sbsS'front'
p284
g1
(cpraw.models.front
Front
p285
g3
NtRp286
(dp287
g35
g37
sS'_comments'
p288
NsS'_path'
p289
S'/'
sbsS'live'
p290
g1
(cpraw.models.helpers
LiveHelper
p291
g3
NtRp292
(dp293
g35
g37
sbsS'inbox'
p294
g1
(cpraw.models.inbox
Inbox
p295
g3
NtRp296
(dp297
g35
g37
sbsS'multireddit'
p298
g1
(cpraw.models.helpers
MultiredditHelper
p299
g3
NtRp300
(dp301
g35
g37
sbsS'_unique_counter'
p302
I0
sg209
g1
(cpraw.config
Config
p303
g3
NtRp304
(dp305
S'username'
p306
g52
sg222
g223
sS'_settings'
p307
(dp308
g306
g52
sg59
g60
sS'password'
p309
g227
sS'user_agent'
p310
VNews Article Downloader /u/JS_Research
p311
sg64
g65
ssS'check_for_updates'
p312
I01
sS'custom'
p313
(dp314
sg61
g63
sg310
g311
sg64
g65
sS'_short_url'
p315
S'https://redd.it'
p316
sg59
g60
sg224
g225
sg309
g227
sS'kinds'
p317
(dp318
g102
g256
sS'message'
p319
g250
sS'redditor'
p320
g253
sS'submission'
p321
g255
sg19
g252
ssg229
g63
sbsS'_read_only_core'
p322
g1
(g40
g3
NtRp323
(dp324
g43
g1
(cprawcore.auth
ReadOnlyAuthorizer
p325
g3
NtRp326
(dp327
g53
Nsg47
Nsg228
Nsg229
Nsg55
g57
sbsg230
g1
(g231
g3
NtRp328
(dp329
g234
Nsg235
Nsg236
Nsg237
NsbsbsS'_authorized_core'
p330
g41
sS'user'
p331
g1
(cpraw.models.user
User
p332
g3
NtRp333
(dp334
g35
g37
sS'_me'
p335
g1
(g254
g3
NtRp336
(dp337
S'is_employee'
p338
I00
sS'has_visited_new_profile'
p339
I00
sS'pref_no_profanity'
p340
I01
sg11
(dp341
sS'is_suspended'
p342
I00
sS'pref_geopopular'
p343
V
sS'_listing_use_sort'
p344
I01
sg19
NsS'is_sponsor'
p345
I00
sS'gold_expiration'
p346
NsS'id'
p347
Ve7dsf19
p348
sS'suspension_expiration_utc'
p349
NsS'verified'
p350
I00
sg27
I00
sS'new_modmail_exists'
p351
NsS'features'
p352
(dp353
Vsearch_public_traffic
p354
(dp355
Vowner
p356
Vsearch
p357
sVvariant
p358
Vnew_search_11
p359
sVexperiment_id
p360
I212
ssVdo_not_track
p361
I01
sVgeopopular_au_holdout
p362
(dp363
Vowner
p364
Vrelevance
p365
sVvariant
p366
Vcontrol_2
p367
sVexperiment_id
p368
I206
ssVshow_amp_link
p369
I01
sVlive_happening_now
p370
I01
sVadserver_reporting
p371
I01
sVgeopopular
p372
I01
sVchat_rollout
p373
I01
sVads_auto_refund
p374
I01
sVlisting_service_rampup
p375
I01
sVmobile_web_targeting
p376
I01
sVdefault_srs_holdout
p377
(dp378
Vowner
p379
Vrelevance
p380
sVvariant
p381
Vtutorial
p382
sVexperiment_id
p383
I171
ssVadzerk_do_not_track
p384
I01
sVusers_listing
p385
I01
sVshow_user_sr_name
p386
I01
sVwhitelisted_pms
p387
I01
sVpersonalization_prefs
p388
I01
sVupgrade_cookies
p389
I01
sVnew_overview
p390
I01
sVnew_report_flow
p391
I01
sVblock_user_by_report
p392
I01
sVadblock_test
p393
I01
sVlegacy_search_pref
p394
I01
sVorangereds_as_emails
p395
I01
sVmweb_xpromo_modal_listing_click_daily_dismissible_ios
p396
I01
sVexpando_events
p397
I01
sVeu_cookie_policy
p398
I01
sVprogrammatic_ads
p399
I01
sVforce_https
p400
I01
sVinbox_push
p401
I01
sVpost_to_profile_beta
p402
I01
sVcrossposting_ga
p403
I01
sVoutbound_clicktracking
p404
I01
sVnew_loggedin_cache_policy
p405
I01
sVshow_secret_santa
p406
I01
sVhttps_redirect
p407
I01
sVsearch_dark_traffic
p408
I01
sVmweb_xpromo_interstitial_comments_ios
p409
I01
sVpause_ads
p410
I01
sVgive_hsts_grants
p411
I01
sVshow_recommended_link
p412
I01
sVmobile_native_banner
p413
I01
sVmweb_xpromo_interstitial_comments_android
p414
I01
sVads_auction
p415
I01
sVgeopopular_se_holdout
p416
(dp417
Vowner
p418
Vrelevance
p419
sVvariant
p420
Vcontrol_2
p421
sVexperiment_id
p422
I224
ssVscreenview_events
p423
I01
sVsubreddit_recommendations_carousel_holdout
p424
(dp425
Vowner
p426
Vrelevance
p427
sVvariant
p428
Vcontrol_2
p429
sVexperiment_id
p430
I239
ssVnew_report_dialog
p431
I01
sVmoat_tracking
p432
I01
sVsubreddit_rules
p433
I01
sVadzerk_reporting_2
p434
I01
sVactivity_service_write
p435
I01
sVads_auto_extend
p436
I01
sVinterest_targeting
p437
I01
sVpost_embed
p438
I01
sVmweb_xpromo_ad_loading_android
p439
(dp440
Vowner
p441
Vchannels
p442
sVvariant
p443
Vcontrol_1
p444
sVexperiment_id
p445
I187
ssVscroll_events
p446
I01
sVmweb_xpromo_modal_listing_click_daily_dismissible_android
p447
I01
sVcrossposting_recent
p448
I01
sVactivity_service_read
p449
I01
ssS'over_18'
p450
I00
sS'is_gold'
p451
I00
sS'is_mod'
p452
I00
sS'has_verified_email'
p453
I00
sS'in_redesign_beta'
p454
I00
sS'has_mod_mail'
p455
I00
sS'oauth_client_id'
p456
VwU1FwN1ugWt7eQ
p457
sS'hide_from_robots'
p458
I00
sS'link_karma'
p459
I1
sg35
g37
sS'inbox_count'
p460
I1
sS'pref_top_karma_subreddits'
p461
NsS'has_mail'
p462
I01
sS'pref_show_snoovatar'
p463
I00
sg104
VJS_Research
p464
sS'created'
p465
F1505744953
sS'_stream'
p466
NsS'gold_creddits'
p467
I0
sS'created_utc'
p468
F1505716153
sS'in_beta'
p469
I00
sS'comment_karma'
p470
I0
sS'has_subscribed'
p471
I00
sg289
S'user/JS_Research/'
p472
sbsbsbsS'_filters'
p473
Nsg466
Nsg288
NsS'_mod'
p474
NsS'_moderator'
p475
Nsg289
S'r/science/'
p476
sbsS'selftext_html'
p477
NsS'selftext'
p478
V
sS'likes'
p479
NsS'suggested_sort'
p480
Vconfidence
p481
sS'user_reports'
p482
(lp483
sS'secure_media'
p484
NsS'is_reddit_media_domain'
p485
I00
sS'link_flair_text'
p486
VNeuroscience
p487
sg347
V6ld70o
p488
sS'banned_at_utc'
p489
NsS'view_count'
p490
NsS'archived'
p491
I00
sS'clicked'
p492
I00
sS'report_reasons'
p493
NsS'title'
p494
VNew study supports feasibility of modeling human morality for machine-based moral decisions.
p495
sS'num_crossposts'
p496
I0
sS'saved'
p497
I00
sS'mod_reports'
p498
(lp499
sS'can_mod_post'
p500
I00
sS'is_crosspostable'
p501
I00
sS'pinned'
p502
I00
sS'comment_limit'
p503
I2048
sS'score'
p504
I233
sS'approved_by'
p505
Nsg450
I00
sS'hidden'
p506
I00
sS'preview'
p507
(dp508
Vimages
p509
(lp510
(dp511
Vsource
p512
(dp513
Vurl
p514
Vhttps://i.redditmedia.com/Cot9CaQFYxeD1WRvhGRJJHybEq9fRzm-eIWodSj1_JI.jpg?s=577e453401292ad0cd5e3fe7ec76c376
p515
sVwidth
p516
I400
sVheight
p517
I118
ssVresolutions
p518
(lp519
(dp520
Vurl
p521
Vhttps://i.redditmedia.com/Cot9CaQFYxeD1WRvhGRJJHybEq9fRzm-eIWodSj1_JI.jpg?fit=crop&crop=faces%2Centropy&arh=2&w=108&s=adcf758a32cf18e2d056f1a804c19e3e
p522
sVwidth
p523
I108
sVheight
p524
I31
sa(dp525
Vurl
p526
Vhttps://i.redditmedia.com/Cot9CaQFYxeD1WRvhGRJJHybEq9fRzm-eIWodSj1_JI.jpg?fit=crop&crop=faces%2Centropy&arh=2&w=216&s=acc95e87def5b72438a06b65740cc626
p527
sVwidth
p528
I216
sVheight
p529
I63
sa(dp530
Vurl
p531
Vhttps://i.redditmedia.com/Cot9CaQFYxeD1WRvhGRJJHybEq9fRzm-eIWodSj1_JI.jpg?fit=crop&crop=faces%2Centropy&arh=2&w=320&s=6a9c8b88957b346b166e095b6b1e510e
p532
sVwidth
p533
I320
sVheight
p534
I94
sasVvariants
p535
(dp536
sVid
p537
VsXeGdM9B7pN72TC9CzGfLylriZLh3M8erk70uM4x3V4
p538
sasVenabled
p539
I00
ssS'thumbnail'
p540
Vhttps://b.thumbs.redditmedia.com/XYmgBA-Ps3PckvfFz2aW1R5keiB1vvKsBujJR4m2pys.jpg
p541
sS'subreddit_id'
p542
Vt5_mouw
p543
sS'whitelist_status'
p544
Vall_ads
p545
sS'edited'
p546
I00
sS'link_flair_css_class'
p547
Vneuro
p548
sS'author_flair_css_class'
p549
NsS'contest_mode'
p550
I00
sS'gilded'
p551
I0
sS'downs'
p552
I0
sS'brand_safe'
p553
I01
sS'secure_media_embed'
p554
(dp555
sS'removal_reason'
p556
NsS'post_hint'
p557
Vlink
p558
sS'stickied'
p559
I00
sg35
g37
sS'can_gild'
p560
I01
sS'thumbnail_height'
p561
I41
sS'parent_whitelist_status'
p562
Vall_ads
p563
sg104
Vt3_6ld70o
p564
sg474
NsS'spoiler'
p565
I00
sS'permalink'
p566
V/r/science/comments/6ld70o/new_study_supports_feasibility_of_modeling_human/
p567
sS'subreddit_type'
p568
Vpublic
p569
sS'locked'
p570
I00
sS'hide_score'
p571
I00
sg465
F1499279166
sS'url'
p572
Vhttp://journal.frontiersin.org/article/10.3389/fnbeh.2017.00122/full
p573
sS'author_flair_text'
p574
NsS'quarantine'
p575
I00
sS'author'
p576
g1
(g254
g3
NtRp577
(dp578
g104
Vrecentfish
p579
sg27
I00
sg11
(dp580
sg344
I01
sg466
Nsg35
g37
sg289
S'user/recentfish/'
p581
sbsg468
F1499250366
sS'subreddit_name_prefixed'
p582
Vr/science
p583
sS'ups'
p584
I233
sg34
NsS'media'
p585
NsS'article_text'
p586
VSelf-driving cars are posing a new challenge to our ethics. By using algorithms to make decisions in situations where harming humans is possible, probable, or even unavoidable, a self-driving car's ethical behavior comes pre-defined. Ad hoc decisions are made in milliseconds, but can be based on extensive research and debates. The same algorithms are also likely to be used in millions of cars at a time, increasing the impact of any inherent biases, and increasing the importance of getting it right. Previous research has shown that moral judgment and behavior are highly context-dependent, and comprehensive and nuanced models of the underlying cognitive processes are out of reach to date. Models of ethics for self-driving cars should thus aim to match human decisions made in the same context. We employed immersive virtual reality to assess ethical behavior in simulated road traffic scenarios, and used the collected data to train and evaluate a range of decision models. In the study, participants controlled a virtual car and had to choose which of two given obstacles they would sacrifice in order to spare the other. We randomly sampled obstacles from a variety of inanimate objects, animals and humans. Our model comparison shows that simple models based on one-dimensional value-of-life scales are suited to describe human ethical behavior in these situations. Furthermore, we examined the influence of severe time pressure on the decision-making process. We found that it decreases consistency in the decision patterns, thus providing an argument for algorithmic decision-making in road traffic. This study demonstrates the suitability of virtual reality for the assessment of ethical behavior in humans, delivering consistent results across subjects, while closely matching the experimental settings to the real world scenarios in question.\u000a\u000aIntroduction\u000a\u000aPrivately owned cars with autopilots first became a reality with a software update which Tesla Motors released to its fleet in October 2015, and many comparable systems will be on the market soon. While initially, these systems are likely to be restricted to highway use, they will eventually make their way into cities, with estimates predicting autonomous vehicles (AVs) dominating road traffic by the 2040s (Marcus, 2012; Litman, 2014). The new technology is expected to reduce the number of car accidents significantly: The German Federal Statistics Agency reports that in 2015, 67% of all accidents with injuries to people were caused by driver misconduct. A 2008 survey by the National Highway Traffic Safety Administration (NHTSA) even showed that human error played a crucial role in 93% of traffic accidents in the US. These numbers outline the enormous potential of self-driving cars regarding road safety. In fact, Johansson and Nilsson (2016) claim that self-driving cars will adjust their driving style and speed such that safe handling of any unexpected event is guaranteed at all times. However, this approach appears unrealistic for many mixed traffic (human and AVs) and inner city scenarios. To ensure absolute safety even in very unlikely events, the car would have to drive in an overly cautious manner, and as a result may be switched off by many drivers, or tempt other drivers to engage in risky overtaking. Other rare events, such as a distracted human driver swerving into the opposite lane, seem very hard to evade altogether under any circumstances. Even when completely taking human drivers out of the equation, we are left with a considerable number of accidents, caused, for instance, by technical or engineering failure (Goodall, 2014b). Altogether, with over a billion cars in operation worldwide, the sheer amount of traffic virtually guarantees that, in spite of the overall expected reduction of accidents, critical situations will occur on a daily basis.\u000a\u000aWith accidents involving autonomous cars being and becoming a reality, ethical considerations will inevitably come into play. Any decision that involves risk of harm to a human or even an animal is considered to be an ethical decision. This includes everyday decisions, e.g., deciding if and when to take a minor risk in overtaking a cyclist. But it also includes quite rare situations in which a collision is unavoidable, but a decision can be made as to which obstacle to collide with. By relying on algorithms to make these decisions, a self-driving car's ethics come pre-defined by the engineer, whether it's done with sophisticated ethical systems or simple rules such as always stay in the lane. This development poses a new challenge to the way we handle ethics. If human drivers are in an accident and make a bad decision from an ethical standpoint, we count in their favor that they had incomplete knowledge of the situation and only fractions of a second to make a decision. Therefore, we typically refrain from assigning any blame to them, morally or legally (Gogoll and Mller, 2017). Algorithms in self-driving cars, on the other hand, can estimate the potential outcome of various options within milliseconds, and make a decision that factors in an extensive body of research, debates, and legislations (Lin, 2013). Moreover, the same algorithms are likely to be used in thousands or millions of cars at a time. Situations that are highly unlikely for an individual car become highly probable over the whole fleet. This enhances the importance of getting it right, and unpreparedness to handle this type of situation may result in a significant number of bad decisions overall.\u000a\u000aUltimately, moral decision-making systems should be considered a necessity for self-driving cars (Goodall, 2014a). The present study addresses the question of how to assess and how to model human moral decision-making in situations in which a collision is unavoidable and a decision has to be made as to which obstacle to collide with. We conducted a virtual reality (VR) study in which participants had to make exactly this type of decision for varying combinations of obstacles, and used the obtained data to train and evaluate a number of different ethical decision-making models. In the next section, we will review the current state of psychological research with respect to moral judgment and decision-making, and derive the outline for the present experiment.\u000a\u000aThe Psychology of Moral Judgment\u000a\u000aThe scenario in this study can be seen as an adaptation of the trolley dilemma, a thought experiment commonly used in research on moral judgment and decision-making, in which a runaway trolley is heading toward a group of five people. The only way to save these five is to pull a lever and divert the trolley onto a different track, killing a single person instead (Thomson, 1985). The utilitarian choice here is to pull the lever and sacrifice one person in order to save five. By contrast, deontologism focuses on the rights of individuals, often putting these ahead of utilitarian considerations. From this perspective, the act of killing a person would be considered morally wrong, even if it means saving five other lives. In a popular alteration of the trolley problem, called the footbridge dilemma, the subject finds themself on a bridge over the tracks with a stranger. Pushing the stranger off the bridge in front of the oncoming train would stop the train and save the five people standing on the track. Interestingly, most people say they would pull the lever in the original trolley dilemma, but only a minority also says they would push the stranger off the bridge in the footbridge dilemma (Greene et al., 2001). An extensive body of psychological research is concerned with the affective, cognitive and social mechanisms underlying this judgment, our ethical intuitions and behavior (Huebner et al., 2009; Christensen and Gomila, 2012; Cushman and Greene, 2012; Waldmann et al., 2012; Avramova and Inbar, 2013). Most prominently, the dual process theory, put forward by Greene et al. (2004), proposes two distinct cognitive systems in competition. The first is an intuitive, emotionally rooted system, eliciting negative affect when behavioral rules are violated, favoring decisions in line with deontological ethics. The second one is a controlled, reasoning-based system, favoring decisions corresponding with utilitarian ethics. Greene's dual-process theory thus explains the different endorsement rates of utilitarian behavior in the trolley and footbridge dilemma by the more emotionally engaging nature of the latter. Pushing a stranger off a bridge instead of pulling a lever requires personal force and uses harm as a means to an end, rather than as a side effect, both increasing the emotionality of the situation, and thus shifting focus to the system favoring deontological ethics (Greene et al., 2001). Similarly, framing a dilemma as more personal (I would do instead of it is acceptable to) and increasing the emotional proximity to the potential victim will also result in fewer utilitarian choices (Greene et al., 2001; Tassy et al., 2013). Neuroscientific evidence is provided by Tassy et al. (2012), showing that disrupting the right dorsolateral prefrontal cortex, associated with emotional processing, increases the likelihood of utilitarian responses. Valdesolo and DeSteno (2006) found an increased probability of utilitarian responses when inducing positive affect, arguing that the positive affect may cancel out the negative affect connected to rule violations.\u000a\u000aHowever, the dual-process theory, based on the emotion-cognition distinction is not undisputed. Cushman (2013) argues that while a distinction between competing processes is necessary, the distinction between affective and non-affective processing is inadequate, since both processes must involve cognition, as well as affective content. Instead, he proposes a distinction based on two cognitive mechanisms borrowed from the field of reinforcement learning. The first is an action-based system, assigning reward values to possible actions in a given situation. These reward values are learned from experience and statically assigned to a given situation-action-pair. The second mechanism is outcome-based and relies on an underlying world model. In simplified terms, it predicts the consequences of the possible actions in a given situation and reassigns the value of the consequence to the action that leads to it. In the trolley dilemma, the outcome-based system would favor utilitarian behavior, and the action-based system would not intervene because the action of pulling a lever is not generally associated with negative reward. Conversely, the action of pushing a person off a bridge is associated with negative reward, thus explaining the lower endorsement rates of utilitarian behavior in the footbridge dilemma. Further evidence in favor of the action vs. outcome distinction in dual-process models is given, e.g., in Cushman et al. (2012) and Francis et al. (2016).\u000a\u000aIn another theory in the realm of moral judgment, Haidt and Graham (2007) aim to explain different views of opposing political camps (liberals and conservatives) with a model of morality based on five factors, and the relative importance of each of these factors to members of the political camps. Finally, based on a large body of neuroscientific evidence, Moll et al. (2008) propose a detailed account of moral emotions as the foundation of our moral judgment. While none of the two entail concrete predictions with respect to moral decision-making in the trolley dilemma and similar scenarios, they demonstrate that the scope of the dual-process theories is limited, and that we are a long way from a comprehensive theory about the cognitive mechanisms governing our moral judgment and behavior.\u000a\u000aVirtual Reality Assessment and Effects of Time Contraints\u000a\u000aWhile most of the aforementioned research relies on abstract, text-based presentations of dilemma situations, a growing number of studies makes use of the possibilities provided by virtual reality (VR) technology. VR, and in particular immersive VR using head-mounted displays (HMDs) and head-tracking, allows assessing moral behavior in a naturalistic way, immersing the subject in the situation, providing much richer contextual information, and allowing for more physical input methods. In an immersive VR version of the trolley dilemma, Navarrete et al. (2012) were able to confirm the utilitarian choice's approval rate of 90%, previously found in text-based studies. Further, they found a negative correlation between emotional arousal and utilitarian choices, in line with the predictions of the dual process theory. In contrast to this, Patil et al. (2014) found both emotional arousal and endorsements of utilitarian choices to be higher in a desktop-VR setting with 3D graphics on a desktop screen than in a text-based setting. While hinting toward a possible distinction between moral judgment and behavior, the results also suggest that features other than emotional arousal play a major role in our moral judgment. The authors argue that the contextual saliency (including a depiction of the train running over the virtual humans) may have shifted the subjects' focus from the action itself toward the outcome of their decision. The tendency to favor utilitarian judgment would then fit Cushman's account of the dual-process theory. In a similar study by Francis et al. (2016), participants were confronted with the footbridge dilemma, either in an immersive VR environment or in a text-based scenario. In the text-based condition, endorsement of the utilitarian choice was low at around 10%, in line with expectations based on previous assessments. In the VR condition, however, subjects opted to push the stranger off the bridge in up to 70% of the trials. These results are again in line with Cushman's account of the dual-process theory, and make a strong case for the notion of moral judgment and moral behavior being distinct constructs. In a different approach, Skulmowski et al. (2014) varied the standard design of the trolley dilemma in multiple ways. First, they virtually placed participants in the trolley's cockpit instead of a bystanders' perspective. Second, they designed the track to split into three and blocked the middle track with a stationary trolley, which had to be avoided. Participants were thus forced to choose between the outside tracks, precluding the deontological option of not intervening in the situation. Third, the subjects had to react within 2.5 s after the obstacles became visible. Finally, in addition to varying the number of people on the available tracks, the authors added a number of trials with only one person standing on either of the available tracks. These differed in gender, ethnicity, and whether the person was facing toward the trolley or away from it. Unsurprisingly, the group was saved in 96% of the the one-vs.-many trials. In the single obstacle trials, significant differences were only found in the gender condition (deciding between a man and a woman), with men being sacrificed in around 58% of the cases.\u000a\u000aThe natural passing of time is a feature inherent to VR studies of this kind. While in principle, it would be possible to pause time in the virtual world, doing so might break immersion and would likely lessen the ecological validity of the obtained results. The previously mentioned studies all imposed some time constraints, but no systematic variation of response time windows was performed. Nevertheless, the dual-process theories would predict time pressure to influence our moral judgment. The action-based system in Cushman's account of the dual-process theory is thought to be simple and quick, while the outcome-based system involves higher cognitive load and is ultimately slower. Greene's account of the dual-process theory suggests that in emotionally engaging dilemmas, the controlled cognitive system needs to override the initial emotional response before making a utilitarian judgment (Greene, 2009). Indeed, increased cognitive load during decision time was shown to increase response times in personal dilemmas, when a utilitarian response was given (Greene et al., 2008). Paxton et al. (2012) showed that moral judgments can be changed with persuasive arguments, but additional time for deliberation was required for the change to occur. To the best of our knowledge, so far only one study systematically varied the length of response time windows. In Suter and Hertwig (2011), participants were either restricted to give a response within 8 s, or they had to first deliberate for 3 min. For high-conflict scenarios, such as the footbridge dilemma, higher time pressure led to fewer utilitarian responses. A second experiment in the same study supports this finding. When no time limitations were given, but one group was instructed to respond intuitively, and the other group was instructed to deliberate before entering a reaction, the intuitive group's response times were a lot shorter than the deliberate group's, and they gave fewer utilitarian responses.\u000a\u000aIn conclusion, VR studies have shown the importance of contextual cues for our decision-making and provide intriguing evidence for a distinction of moral judgment and behavior. Moreover, time constraints, as an inherent feature to VR setups, have been recognized as a factor in our moral decision-making. There is evidence suggesting that longer deliberation may facilitate utilitarian decisions in certain complex scenarios, but we still lack a systematic analysis of the influence of time pressure on moral judgment.\u000a\u000aModeling of Human Moral Behavior\u000a\u000aAn important criterion that an ethical decision-making system for self-driving cars or other applications of machine ethics should meet is to make decisions in line with those made by humans. While complex and nuanced ethical models capable of replicating our cognitive processes are out of reach to date, simpler models may deliver adequate approximations of human moral behavior, when the scope of the model is confined to a small and specific set of scenarios. Value-of-life-based models stand to reason as a possible solution for any situation in which a decision has to be made as to which one of two or more people, animals, other obstacles, or groups thereof to collide with. An account of a value-of-life model that is focused on a person's age is given by Johansson-Stenman and Martinsson (2008). The authors conducted a large-scale survey in which people had to indicate in several instances, on which of two road-safety-improvement measures they would rather spend a given budget. The available measures differed with respect to the age and expected number of people that would be saved, as well as whether the ones saved would be pedestrians or car drivers. The authors used a standard probit regression model to fit the observed data, and found that not the number of saved lives, but rather the number of saved life-years to be the most important factor in the subjects' decision, allowing for specific values of life to be assigned to each age group. Beyond this, they found pedestrians to be valued higher than car drivers of the same age, indicating consent as a factor in the valuation. While discriminating between potential human crash victims based on age, or possibly gender, is unlikely to gain general public acceptance, Goodall (2014a) suggests using value-of-life scales in cases where higher-level rules fail to provide the system with clear instructions. Furthermore, if we take animals into account, value-of-life scales stand to reason as a way of dealing with vastly differing probabilities. When a decision has to be made between killing a dog with near certainty and taking a 5% risk of injuring a human, how should the algorithm decide? We don't seem to take much issue with assigning different values of life to different species, and a system favoring pets over game or birds might be acceptable in the public eye. While this makes the case for at least some form of value-of-life model, it remains to be seen to what extent such models are able to capture the complexity of human ethical decision-making.\u000a\u000aDeriving and Outlining the Experiment\u000a\u000aAs discussed in previous sections, our moral judgment is highly dependent on a wide variety of contextual factors, and there is no ground truth in our ethical intuitions that holds irrespective of context. We thus argue that any implementation of an ethical decision-making system for a specific context should be based on human decisions made in the same context. To date, our limited understanding of the cognitive processes involved prevents us from constructing a comprehensive ethical model for use in critical real-world scenarios. In the context of self-driving cars, value-of-life scales stand to reason as simple models of human ethical behavior when a collision is unavoidable, and an evaluation of their applicability in this context is the main focus of this study.\u000a\u000aTo this end, participants were placed in the driver's seat of a virtual car heading down a road in a suburban setting. Immersive VR technology was used to achieve a maximum in perceived presence in the virtual world. A wide variety of different obstacles, both animate and inanimate, were randomly paired and presented in the two lanes ahead of the driver. Participants had to decide which of the two they would save, and which they would run over. Since prolonged sessions in immersive VR can cause nausea and discomfort, we opted for a pooled experimental design with short sessions of 9 trials per condition and participant. We thus pooled the trials of all participants, and used this data set to train three different logistic regression models to predict the lane choice for a given combination of obstacles. (1) The pairing model uses each possible pairing of obstacles as a predictor. Here, a given prediction reflects the frequency with which one obstacle was chosen over the other in the direct comparisons. Since an obstacle is not represented with a single numerical value, the pairing model is not a value-of-life model, but serves as a frame of reference. (2) The obstacle model assigns one coefficient to each obstacle and uses the obstacles' occurrences as predictors. We interpret these coefficients as the obstacle's value of life. (3) The cluster model uses only one coefficient per category of obstacles, as they resulted from a clustering based on the frequency with which each obstacle was hit.\u000a\u000aWe compare the three different models to test a set of hypotheses. Our first hypothesis was that a one-dimensional value-of-life-based model (i.e., the obstacle model) fully captures the complexities of pairwise comparisons. The obstacle model should thus be as accurate as the pairing model. This would mean that our ethical decisions can be described by a simple model in which each possible obstacle is represented by a single value, and the decision which obstacle to save is based only on these respective values. We further hypothesized (Hypothesis 2) that within-category distinctions, for example, between humans of different age, are an important factor in the decisions. Specifically, the obstacle model should prove to be superior to the cluster model. Furthermore, since a certain amount of time pressure is inherent to naturalistic representations of this scenario, we investigated its influence on the decisions by varying the time to respond in two steps, giving participants a response window of either 1 or 4 s. We found 4 s of decision time to induce relatively little time pressure in the used scenario, while one second still left a sufficient amount of time to comprehend and react. We hypothesized (Hypothesis 3) that more errors would be made under increased time pressure, and that ethical decisions would thus be less consistent across subjects in these trials. The dual-process theories would predict a higher endorsement of utilitarian choices with more time to deliberate (i.e., in the slow condition). However, for comparisons of single obstacles, there is no clearly defined utilitarian choice. If anything, basing the decision in human vs. human trials on a person's expected years to live could be considered utilitarian, and is partly covered in Hypothesis 2. Moreover, the omission of a lane change, despite running over the more valuable obstacle, could be interpreted as a deontological choice, but we didn't formulate any directional hypothesis regarding this factor prior to the study.\u000a\u000aMethods\u000a\u000aThe experiment ran in a 3D virtual reality application, implemented with the Unity game engine, using the Oculus Rift DK2 as the head-mounted display. The audio was played through Bose QC25 and Sennheiser HD215 headphones throughout the experiment. The participants were sitting in the driver's seat of a virtual car heading down a suburban road. Eventually, two obstacles, one on either lane, blocked the car's path and the participants had to choose which of the two obstacles to hit. Using the arrow keys on the keyboard, the participants were able to switch between the two lanes at all times, up to a point approximately 15 m before impact. This way, we provided a high level of agency, intended to closely resemble manual car driving, while making sure the decision could not be avoided by zig-zagging in the middle of the road or crashing the car before reaching the obstacles. We used 17 different obstacles from three different categories, i.e., inanimate objects, animals, and humans as single obstacles, as well as composite obstacles. An empty lane was additionally used as a control. For each trial, two of the 17+1 obstacles were pseudo-randomly chosen and allocated to the two lanes, as was the starting lane of the participant's car. A wall of fog at a fixed distance from the participant's point of view controlled the exact time of the obstacle's onset. We varied the length of the reaction time window by varying the fog distance and car speed, resulting in a window of 4 s for the slow, and one second for the fast condition. To indicate how much time was left to make a decision at each point in time, a low-to-high sweep sound was played as an acoustic cue. The sound started and ended on the same respective frequencies in both conditions, thus sweeping through the frequency band quicker in the fast condition. After the decision time window had ended around 15 meters before impact, the car kept moving, completing any last instant lane changes. Right before impact, all movement was frozen, all sounds stopped, and the screen faded to black, marking the end of a trial. Figure 1 illustrates the chronological progression of the trials in the fast and slow condition, and gives an overview of all obstacles. The fast and slow trials were presented in separate blocks of 9 trials each. Two more blocks of trials were presented but not analyzed in the current study, and all four blocks were presented in randomized order. We chose obstacle pairings such that each obstacle typically appeared once per subject and block. The frequency of appearance of all 153 possible pairings, as well as the lane allocations and starting lanes, were balanced over all subjects.\u000a\u000aFIGURE 1\u000a\u000aFigure 1. (Left) Overview of the experimental setting. (Middle) Timelines of the slow and fast conditions. (Right) Overview of all obstacles used. Colors indicate cluster assignments.\u000a\u000aSample and Timeline\u000a\u000aOur sample consists of 105 participants (76 male, 29 female) between the age of 18 and 60 (mean: 31) years. We excluded one subject who reported a partial misunderstanding of the task, as well as three outliers whose decisions were the opposite of the model prediction (see below) in more than 50% of their respective trials. Most of the participants were university students or visitors of the GAP9 philosophy conference. Before participating, we informed all subjects about the study, potential risks and the option to abort the experiment at any time. They were also informed that the external screens would be turned off during the experiment, so that others could not observe their decisions. After signing a consent form, they were asked to put on the HMD and headphones, and then received all further information within the application. As a first task, they had to complete a training trial, avoiding three pylons by alternating between the lanes. Upon hitting a pylon, the training trial was repeated until completed without error. This procedure gave participants a chance to familiarize themselves with the VR environment, and it made sure they had understood the controls before entering the experimental trials. The study conformed to the Code of Ethics of the American Psychological Association, as well as to national guidelines, and was approved by the Osnabrck University's ethics committee.\u000a\u000aResults\u000a\u000aWe pooled all data and did not consider between-subject differences in the analysis. In the experimental trials, the mean number of lane switches per trial was 0.816 in the slow and 1.037 in the fast condition. We estimated error rates for both conditions, using trials in which one of the lanes was empty. Hitting the only obstacle in such a trial was considered an error, as we find it safe to assume that the outcome in these trials is a result of inadvertently pressing the wrong button, instead of a meaningful decision. This event occurred in 2.8% of all trials containing an empty lane in the slow condition, and in 12.0% of the relevant trials in the fast condition. As a frame of reference, the chance level for this was at 50%.\u000a\u000aBehavioral Models\u000a\u000aAll models used in the present study were logistic regression models, using the occurrence of obstacle pairings, individual obstacles or clusters, i.e., obstacle categories (see below) in a particular trial as predictors for the lane choice. Furthermore, we added a constant offset and the trial's starting lane as predictors to all models. The constant offset allowed us to detect potential biases in the overall lane preference (left or right). Such a bias could occur, for example, when participants are used to right-hand traffic and feel that using the right-hand lane is generally more acceptable. Including the starting lane as a predictor allowed us to detect a bias to stay in the respective trials' starting lanewe would label this an omission biasor to move away from the starting lane, i.e., a panic reaction bias.\u000a\u000aA model's predicted probability of choosing to drive in the left lane is given by p ( Y = left | X ) = 1 1 + exp ( - X ) , with X being the model-specific representation of a particular trial.\u000a\u000aIn the pairing model, a trial is represented as X p = c i + s s +  b , where  i is the coefficient for obstacle pairing i (e.g., {boy, woman}), c  {1, 1} is the lane configuration in the respective trial (e.g., 1 if the boy is in the left lane, 1 if the woman is in the left lane),  s is the starting lane coefficient, s  {1, 1} is the starting lane 1 if the starting lane is right, and  b is the coefficient for the lane bias. The model is thus making a prediction based on a general preference for one of the lanes, the starting lane of the respective trial, and which of the 153 possible pairings is presented in the trial, resulting in 155 parameters in total. Figure 2 left shows the predictions of the pairing model. Since each pairing of obstacles has its own free parameter, the model allows for intransitive and other complex relations between the obstacles. For example, in the slow condition, the pairing model deems the goat to be more valuable than the boar, and the boar to be more valuable than the deer, but the goat to be less valuable than the deer. Consequentially, there is no single value of life for an obstacle in the pairing model. An obstacle's value is only defined relative to each of the other obstacles.\u000a\u000aFIGURE 2\u000a\u000aFigure 2. Model predictions. (Top) Slow condition, (Bottom) fast condition, (Left) pairing model, (Middle) obstacle model, (Right) cluster model. Colors indicate the probability of saving the row-obstacle (y-axis) and sacrificing the column-obstacle (x-axis). Pink, green, blue, and black bars indicate cluster assignments based on agglomerative clustering in the slow condition (see Figure 3). For means of comparability, the cluster model in the fast condition was fit with the semantic cluster assignments from the slow condition.\u000a\u000aIn the obstacle model, a trial is represented as X o =  ro   lo + s s +  b , with  ro and  lo being the coefficients for the right and left obstacle in the respective trial. Each obstacle is thus represented by a single characteristic value or value of life. All pairwise comparisons result directly from a subtraction of the respective two values of life. Thus, when sorting all obstacles according to their value of life on the abscissa and ordinate, the order in the vertical and horizontal direction is strictly monotonous (Figure 2, middle). Since there are 18 individual obstacles, the model has a total of 20 parameters, including the lane bias and starting lane coefficients.\u000a\u000aSimilarly, in the cluster model, a trial is represented as X c =  rc   lc + s s +  b , with  rc and  lc being the coefficients of the clusters that the obstacles are assigned to. We performed bottom-up clustering and subsequent model selection to derive the ideal number of clusters and cluster allocations of all presented obstacles for the cluster model (see Figure 3). Logistic regression models were first constructed and fitted for all possible numbers of clusters, ranging from 17 to 1. We then performed the model comparison via the Bayesian Information Criterion (BIC). In the slow condition, the five clusters model was found to be the best of the cluster models. Notably, its cluster allocations are perfectly in line with a categorization in none, inanimate objects, animals, humans, and groups of humans and animals. In the fast condition, a four cluster solution was found to be ideal, and its cluster allocations don't align perfectly with the aforementioned semantic categorization. This is likely the result of the higher error rate in the fast condition. In order to still allow for a comparison between both conditions, we chose to use the aforementioned semantic categorization in five clusters for the fast condition, as well. For both conditions, the cluster model thus has five parameters for the obstacle clusters, resulting in a total of only seven parameters, including the lane bias and starting lane coefficients. Figure 2 right shows its predictions in the slow condition. The model uses only one free parameter per cluster of obstacles, resulting in a block structure. Since all obstacles within a cluster are considered equal in value of life, the difference in the value of life is always exactly zero for within-cluster comparisons. Those decisions, therefore, depend entirely on the starting lane and overall lane preference.\u000a\u000aFIGURE 3\u000a\u000aFigure 3. Dendrogram of bottom-up clustering, based on the observed frequencies with which each obstacle was spared (saved), for the slow and fast condition separately.\u000a\u000aAll models were fitted using the logistic regression algorithm in the scikit-learn (version 0.17.1) toolbox for Python. We set the regularization strength to a very low value of 109 and based the model selection on prediction accuracy via 10-fold cross-validation, as well as the Bayesian Information Criterion.\u000a\u000aPairing Model vs. Obstacle Model\u000a\u000aIn a first step, we compare the pairing and the obstacle models. When modeling the training data set, models with a (much) higher number of free parameters can describe the data better. However, in cross-validation, potential overfitting can lead to a reduced performance of the more detailed model. Indeed, with a prediction accuracy of 91.64% in the slow condition and 80.75% in the fast condition, the obstacle model is slightly superior to the detailed pairing model, with prediction accuracies of 89.33% and 78.77%, respectively. Despite our extensive data set with 909 trials per condition, the large number of parameters in the pairing model causes overfitting. This find translates to a much larger BIC value for the pairing model (see Table 1). Thus, our results strongly favor the obstacle model for its lower complexity and reduced risk of overfitting. This result, in combination with the high prediction accuracy of the obstacle model in the slow condition, confirms our first hypothesis, i.e., one-dimensional value-of-life-based models can adequately capture the ethical decisions we make in real life scenarios.\u000a\u000aTABLE 1\u000a\u000aTable 1. BIC-values and prediction accuracy based on 10-fold cross-validation for the three models in the slow and fast condition.\u000a\u000aObstacle Model vs. Cluster Model\u000a\u000aIn the slow condition, the obstacle model's rankings of coefficient values within the categories mostly make sense, intuitively. For example, children are assigned higher values than adults (boy: 2.76, male adult: 2.12, corresponding to a 65.5% chance of saving the boy in a direct comparison with a male adult). Further, the dog is consistently found to be the most valuable of the animals. The prediction accuracies, however, are essentially even between the obstacle model (91.64%) and the cluster model (91.20%), with the cluster model scoring the lower BIC value, due to the reduced number of parameters (see Table 1). These findings are repeated in the fast condition. Prediction accuracies of the obstacle and cluster models are very close to each other (80.75 and 80.53%), and in terms of BIC values the cluster model is superior. We thus have to reject our second hypothesis, because the cluster model with five clusters is selected as superior to the obstacle model. In other words, we found no particular advantage of using obstacle-based predictors instead of category-based predictors.\u000a\u000aBiases\u000a\u000aTo assess the two bias predictors' importance for the model, we ran another model comparison for three additional versions of the cluster model. All three additional versions were based on the above model, but the first variant dropped the starting lane predictor, the second variant dropped the lane bias, and the third variant dropped both predictors. In the slow condition, the cluster model omitting the lane bias, but including the starting lane predictor, scored the lowest BIC value of all models. Its prediction accuracy is the same as that of the previously assessed cluster model with both additional predictors (91.20%), making it the best explanatory model for the observations made in the slow condition (see Table 1). This is also reflected in the respective coefficients. When including the predictor of the lane bias, it was fit to a value of 0.15. The low value indicates only a very weak tendency to the left lane, which makes no significant contribution to the model fit. Thus, even in this rather realistic scenario, participants treated both lanes as equally valid driving lanes. The starting lane predictor was fitted to -0.47, indicating a reluctance to switch lanes in the face of a decision, constituting an omission bias. We can roughly quantify the extent of this reluctancy as being rather small, since coefficient differences between categories are in all cases magnitudes higher. The specific starting lane in a trial would therefore not affect the decisions in between category comparisons. It does, however, play a role in within-category decisions, as evidenced by the 4.8% gap in overall prediction accuracy between the cluster models with and without the starting lane as a predictor. In the fast condition, the best model, both in terms of prediction accuracy as well as BIC score, is the one omitting both bias predictors (see Table 1). By omitting the bias predictors, the prediction accuracy increases from 80.53 to 82.29%, exposing an overfit in the more complex model. In conclusion, the analysis of the bias predictors found lane preference to have no substantial influence on the decisions made in this paradigm, but did reveal an omission bias when facing similarly valued obstacles in the slow condition.\u000a\u000aInfluence of Increased Time Pressure\u000a\u000aWe will now turn to a direct comparison of the slow and fast condition, to evaluate the effects of increased time pressure. The most notable difference between the two conditions is the (estimated) error rate of 12.0% in the fast condition, marking a four-fold increase from the slow condition. As for the cause of the errors, we would expect an increased omission bias if the errors were caused by a mere failure to react in time. Interestingly, this was not the case. Instead, we found an omission bias only in the slow and not in the fast condition, indicating that errors in the fast condition were equally a result of staying in and switching into the lane of the more valuable obstacle. A major increase in error rate also substantially decreases the expected prediction accuracy even for a perfect model. This is reflected in the prediction accuracies for models in the fast condition, which are on average roughly 10% below those for the corresponding models in the slow condition.\u000a\u000aIn the cluster analysis, we found a four cluster model to yield the lowest BIC values in the fast condition, instead of the five cluster model found to be ideal in the slow condition. Moreover, the cluster assignments for some of the obstacles are also different, and do no longer match the semantic categories perfectly (see Figure 2). These findings are consistent with the influence of increased noise in the data, and can therefore also be ascribed to the increased error rates. Since there is no matching cluster model for both the slow and fast condition, we included a comparison of the cluster models based on the semantically defined categorizations in Figure 4, but decided to focus on the obstacle model in the remainder of this comparison. In the obstacle model, the coefficient range in the fast condition was reduced to 50% of that in the slow condition (see Figure 4). Specifically, the obstacles on the extreme sides of the spectrumthe empty lane and the groups of humans and animalsaren't separated well from the adjacent obstacle categories. To statistically confirm this observed difference, we used a nested model approach with log-likelihood ratio tests. For the nested model, we fitted the joint dataset of fast and slow conditions to the obstacle model using 19 predictors, i.e., the 18 obstacles plus the starting lane. For the larger nesting model, we added a second set of 19 predictors. These 19 were duplicates of the first 19 predictors, but were fitted only on the slow condition trials. Together, these two sets formed a model with 38 predictors in total. The log-likelihood ratio test between the nesting and nested model was significant (p = 0.037), showing that the reduction in parameters between the two significantly reduces model accuracy. In other words, the difference between the two conditions is large enough to justify the use of two completely separate sets of parameters to describe them. This confirms our third hypothesis, i.e., increased time pressure significantly decreases the consistency in the answering patterns.\u000a\u000aFIGURE 4\u000a\u000aFigure 4. (Left) Value-of-life coefficients by condition. Pictograms and colors indicate the categories empty lane, inanimate objects, animals, humans, and groups of humans and animals (left to right). Starting lane coefficients depicted as gray bars. (Right) Relative frequency of saving the empty lane object, used as error rate estimates, for fast and slow condition separately.\u000a\u000aAnother notable difference between the two conditions is that we no longer observe a bias toward sacrificing the male adults in direct stand-offs with female adults. Instead, participants saved males in 4 out of 7 cases in the fast condition. The previously speculated tendency toward social desirability would likely rely on slower cognitive processes, and thus not come into effect in fast-paced intuitive decisions.\u000a\u000aDiscussion\u000a\u000aWe investigated the capability of logistic regression-based value-of-life models to predict human ethical decisions in road traffic scenarios with forced choice decisions, juxtaposing a variety of possible obstacles, including humans, animals, and inanimate objects. The analysis incorporated various contextual and psychological factors influencing our moral decision-making in these situations, and examined in particular the effects of severe time pressure.\u000a\u000aOur first hypothesis was that a one-dimensional value-of-life-based model fully captures the complexities of pairwise comparisons. With prediction accuracies well above 90% in the slow condition, and clearly outperforming the more complex pairing model, the obstacle model proved to be capable of accurately predicting the moral decisions made in the pairwise comparisons. The first hypothesis was thus confirmed. Note that since we used a wide range of obstacles, we cannot preclude some more complex effects happening on a more detailed level. One possible example of such an effect is the following: In the slow condition, the obstacle model shows male and female adults to have comparable value-of-life coefficients with a slight advantage for the males (2.12 vs. 1.79), predicting a 41.8% chance of sacrificing the male adult in a direct comparison. This prediction is based on all the trials it has seen, i.e., the full dataset including all possible combinations of the 18 obstacles. Still, adult males were actually sacrificed in 4 out of the 5 cases (80%) of direct comparisons between male and female adults. This observation is in line with Skulmowski et al. (2014), who also found males to be sacrificed more often in a direct comparison. Interestingly, the authors found the tendency to sacrifice males to be correlated with a general tendency to answer according to social desirability. In our study, the tendency to sacrifice males only pertains to the slow and not to the fast condition, which makes sense, if we assume that the effect is rooted in a tendency toward social desirability. Considerations of social desirability could be construed as part of the outcome-based system in Cushman's account of the dual-process theory, which is thought to be the slower one of the two processes. However, the low number of direct comparisons this figure is based on, and the exploratory nature of this find, dictate caution with respect to its interpretation. We consider it a leverage point for future research, but not a major result of this study.\u000a\u000aOur second hypothesis was that within-category distinctions, for example between humans of different age, are an important factor in the decisions. This hypothesis could not be confirmed in this study, as the obstacle model failed to show an advantage over the cluster model in describing the collected data. However, there are hints at a meaningful structure within the clusters. For example, the obstacle model found children to have higher values than adults, and the dog, as the only common pet among the animals shown, to have the highest value within the animal cluster. Thus, given a larger data basis, we would still expect within-category distinctions to improve the predictions made by value-of-life models. In particular, we would expect age to play a role in human vs. human comparisons. Surveys by Cropper et al. (1994) and Johansson-Stenman and Martinsson (2008) have previously shown that the value we assign to someone's life decreases considerably with the person's age. To what degree these judgment-based findings would also be reflected in assessments of behavior is unclear, since judgment and behavior can yield dramatically different outcomes (Patil et al., 2014; Francis et al., 2016). Based on our findings, we speculate that the difference in value-of-life between people of different ages may be less pronounced in behavioral assessments, but more data is needed to clarify this point.\u000a\u000aIrrespective of the exact outcome of such assessments, systems discriminating based on age, gender or other factors may be considered unacceptable by the public, as well as by lawmakers. Nevertheless, the idea of weighing lives against one another isn't generally rejected. As Bonnefon et al. (2016) showed, a majority of people would prefer a self-driving car acting in a utilitarian manner, at least when it isn't themselves, who are being sacrificed for the greater good. Independent of whether or not human lives should be weighed against one another, assigning different values of life to animals even seems to be the logical choice, judging from how differently we treat different species of animals in other aspects of life. Value-of-life models based on species would allow us to differentiate between common pets and other animals, and would give us a tool to deal with situations in which the death of an animal could be avoided by taking a minor risk of harm to a human.\u000a\u000aOur third hypothesis was that ethical decisions would be less consistent across and within subjects when the time to react is reduced. This hypothesis was confirmed. The error rate was drastically increased, the cluster analysis revealed fewer clusters with slightly different cluster assignments, and the range of value-of-life coefficients was significantly reduced. However, we cannot deduct from our data whether the decisions made under time-pressure are in fact less clear-cut than decisions formed with more time for deliberation, or if the effect can be fully explained by the increased error rate. Still, a full second of time to react is a lot more than we typically encounter in real-life scenarios of this kind, and the weak consistency in the decision patterns is a sign that we are ill-equipped to make moral decisions quickly, even when the situation comes expectedly. We therefore argue that, under high time pressure, algorithmic decisions can be largely preferable to those made by humans.\u000a\u000aAnother noteworthy difference between the fast and slow condition concerns the omission bias, which we only found in the slow, but not in the fast condition. Participants were thus less likely to switch lanes and interfere in the situation when given more time to decide. This fact can be interpreted as a sign of a more deontological reasoningchoosing not to interfere in the situation, and possibly trying to reduce one's own guilt despite causing greater damage as a result. A tendency toward deontological reasoning with more time, however, conflicts with both Greene's and Cushman's accounts of the dual-process theory, as well as, e.g., Suter and Hertwig (2011), who found that more time to decide will cause a shift toward utilitarian responses. One possibly decisive difference between the present study, and most other studies touching on the aspect of time in moral decision-making, is the type of scenario used and the corresponding absolute response times. Typically, the scenarios used are relatively complex moral dilemmas, and response times lie in the 810 s range for short, and up to several minutes in the longer or unconstrained conditions (Greene et al., 2008; Suter and Hertwig, 2011; Paxton et al., 2012). In contrast, the reaction time windows of 4 and 1 s used in the present study rather represent a distinction between short deliberation and pure intuition. The fast condition may thus fall out of the dual-process theories scope.\u000a\u000aIn this study, we purposefully constructed a simple scenario with clearly defined outcomes, featuring the variables necessary to fit value-of-life models. With the general applicability of these value-of-life models established, a number of ensuing questions arise. For example, what influence a person's emotional and cognitive features have on their decision, how different probabilities of a collision or different expected crash severities affect our judgment, and how groups of multiple people or animals should be treated in such models. Moreover, the option of self-sacrifice has been prominently discussed in literature (Lin, 2014; Greene, 2016; Gogoll and Mller, 2017; Spitzer, 2016), and was assessed via questionnaire in Bonnefon et al. (2016), but hasn't been included in behavioral studies so far. We speculate that immersion and perceived presence may have a particularly strong influence on decisions that touch upon one's own well-being. Beyond this, considerations of fairness need to be addressed as wellfor example, if one person is standing on a sidewalk and another has carelessly stepped onto the street. While the choice of a wide range of obstacles has proven helpful in understanding the big picture, more research is needed to answer open questions about effects happening within the categories. The design choices we made allowed us to focus on the applicability of value-of-life models, but the present study does not provide a fleshed-out model for implementation in self-driving cars. Instead, it constitutes a starting point from which to investigate systematically, how a variety of other factors may influence our moral decisions in this type of scenario and how they could be implemented.\u000a\u000aA limiting factor for this study is the use of only one instance of each of the presented obstacles. We tried to select and create 3D models that are as prototypical as possible for their respective classes, but we cannot rule out that the specific appearance of the obstacles may have had an impact on the decisions, and by extension, the coefficient values assigned to the obstacles. Future studies or assessments that put more emphasis on the interpretation of single value-of-life coefficients, should include a variety of instances of each obstacle. Furthermore, larger and explicitly balanced samples would be needed to obtain models sufficiently representative of a society's moral judgment. Another fair point of criticism concerns the plausibility of the presented scenario. There was no option of braking during up to 4 s of decision time, and the car was keyboard-controlled and could only perform full lane switches. While there were good reasons for these design choices, namely to allow for enough decision time and to enforce a clear decision based on an unambiguous scenario, they limit the virtual world's authenticity and may hinder the subjects' immersion. Unfortunately, this issue seems unavoidable in controlled experimental settings. We believe that the virtual world implemented for this study nevertheless fulfills a high standard of authenticity overall, and, under the given constraints, illustrates the scenarios in question as close to reality as currently possible.\u000a\u000aFuture studies should further investigate the role of the presentation mode in this specific context. We argue that based on moral dilemma studies, a distinction between judgment and behavior may be justified. However, it remains to be seen if there is a seizable difference for specifically the kind of situations used in this study that justifies the special effort that goes into the design of a virtual reality environment. Finally, based on our findings, the influence of time pressure could be assessed in greater detail, expanding the considered time frames beyond the 1-4 s range.\u000a\u000aConclusion\u000a\u000aWe argue that the high contextual dependency of moral decisions and the large number of ethically relevant decisions that self-driving cars will have to make, call for ethical models based on human decisions made in comparable situations. We showed that in the confined scope of unavoidable collisions in road traffic, simple value-of-life models approximate human moral decisions well. We thus argue that these models are a viable solution for real world applications in self-driving cars. With respect to trust in the public eye, their simplicity could constitute a key advantage over more sophisticated models, such as neural networks. Furthermore, regression models can include additional factors such as probabilities of injuries for the parties involved, and help to make reasonable decisions in situations where these differ greatly. They also provide an easy option to deal with a vast number of possible obstacles, by testing a few and making reasonable interpolations, e.g., for people of different age, taking away the requirement of assessing each conceivable obstacle individually. That being said, the modeling of within-cluster differences, e.g., between humans of different ages or between different species of animals, failed to improve upon the rather coarse cluster models. We further found time pressure, as an inherent feature to naturalistic portraits of the scenario in question, to considerably decrease the consistency in the decision patterns and call for more investigation of the effect of time pressure on moral decision-making. Overall, we argue that this line of research, despite being met with some skepticism (Johansson and Nilsson, 2016), is important to manufacturers and lawmakers. The sheer expected number of incidents where moral judgment comes into play creates a necessity for ethical decision-making systems in self-driving cars (Goodall, 2014a). We therefore hope to see more efforts toward establishing a sound basis for the methodology of empirically assessing human ethics in the future, as the topic is becoming increasingly important with more advances in technology.\u000a\u000aAuthor Contributions\u000a\u000aLS: Leading role in planning, implementation, data acquisition and data analysis. Writer of the paper. RG: Involved in planning, data acquisition and data analysis. Gave feedback to earlier versions of the paper. GP and PK: Involved in planning, data acquisition and data analysis, and gave feedback to earlier versions of the paper.\u000a\u000aConflict of Interest Statement\u000a\u000aThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\u000a\u000aAcknowledgments\u000a\u000aThe authors would like to thank Jannes Nagel for the implementation of the Unity app, our research assistants Artur Czeszumski, Anja Faulhaber, Nicole Knodel, Maria Sokotushchenko, Lisa Steinmetz, and Lisa-Marie Vortmann for their help during the data acquisition phase. We acknowledge support by Deutsche Forschungsgemeinschaft (DFG) and Open Access Publishing Fund of Osnabrck University.\u000a\u000aReferences\u000a\u000aAvramova, Y. R., and Inbar, Y. (2013). Emotion and moral judgment. Wiley Interdiscip. Rev. Cogn. Sci. 4, 169178. doi: 10.1002/wcs.1216 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aBonnefon, J.-F., Shariff, A., and Rahwan, I. (2016). The social dilemma of autonomous vehicles. Science 352, 15731576. doi: 10.1126/science.aaf2654 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aChristensen, J. F., and Gomila, A. (2012). Moral dilemmas in cognitive neuroscience of moral decision-making: a principled review. Neurosci. Biobehav. Rev. 36, 12491264. doi: 10.1016/j.neubiorev.2012.02.008 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aCropper, M. L., Aydede, S. K., and Portney, P. R. (1994). Preferences for life saving programs: how the public discounts time and age. J. Risk Uncertain. 8, 243265. doi: 10.1007/BF01064044 CrossRef Full Text | Google Scholar\u000a\u000aCushman, F. (2013). Action, outcome, and value a dual-system framework for morality. Pers. Soc. Psychol. Rev. 17, 273292. doi: 10.1177/1088868313495594 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aCushman, F., Gray, K., Gaffey, A., and Mendes, W. B. (2012). Simulating murder: the aversion to harmful action. Emotion 12:2. doi: 10.1037/a0025071 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aCushman, F., and Greene, J. D. (2012). Finding faults: how moral dilemmas illuminate cognitive structure. Soc. Neurosci. 7, 269279. doi: 10.1080/17470919.2011.614000 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aFrancis, K. B., Howard, C., Howard, I. S., Gummerum, M., Ganis, G., Anderson, G., et al. (2016). Virtual morality: transitioning from moral judgment to moral action? PLoS ONE 11:e0164374. doi: 10.1371/journal.pone.0164374 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aGogoll, J., and Mller, J. F. (2017). Autonomous cars: in favor of a mandatory ethics setting. Sci. Eng. Ethics 23, 681700. doi: 10.1007/s11948-016-9806-x PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aGoodall, N. J. (2014a). Ethical decision making during automated vehicle crashes. Transport. Res. Record J. Transport. Res. Board 2424, 5865. doi: 10.3141/2424-07 CrossRef Full Text | Google Scholar\u000a\u000aGoodall, N. J. (2014b). Machine ethics and automated vehicles, in Road Vehicle Automation, eds G. Meyer and S. Beiker (Cham: Springer International Publishing), 93102. doi: 10.1007/978-3-319-05990-7_9 CrossRef Full Text | Google Scholar\u000a\u000aGreene, J. D. (2009). The cognitive neuroscience of moral judgment, in The Cognitive Neurosciences, 4th Edn., ed M. Gazzaniga (MIT Press), 987999.\u000a\u000aGreene, J. D. (2016). Our driverless dilemma. Science 352, 15141515. doi: 10.1126/science.aaf9534 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aGreene, J. D., Morelli, S. A., Lowenberg, K., Nystrom, L. E., and Cohen, J. D. (2008). Cognitive load selectively interferes with utilitarian moral judgment. Cognition 107, 11441154. doi: 10.1016/j.cognition.2007.11.004 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aGreene, J. D., Nystrom, L. E., Engell, A. D., Darley, J. M., and Cohen, J. D. (2004). The neural bases of cognitive conflict and control in moral judgment. Neuron 44, 389400. doi: 10.1016/j.neuron.2004.09.027 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aGreene, J. D., Sommerville, R. B., Nystrom, L. E., Darley, J. M., and Cohen, J. D. (2001). An fMRI investigation of emotional engagement in moral judgment. Science 293, 21052108. doi: 10.1126/science.1062872 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aHaidt, J., and Graham, J. (2007). When morality opposes justice: conservatives have moral intuitions that liberals may not recognize. Soc. Just. Res. 20, 98116. doi: 10.1007/s11211-007-0034-z CrossRef Full Text | Google Scholar\u000a\u000aHuebner, B., Dwyer, S., and Hauser, M. (2009). The role of emotion in moral psychology. Trends Cogn. Sci. 13, 16. doi: 10.1016/j.tics.2008.09.006 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aJohansson, R., and Nilsson, J. (2016). Disarming the trolley problemwhy self-driving cars do not need to choose whom to kill, in Workshop CARS 2016 - Critical Automotive Applications: Robustness and Safety, ed M. Roy (Gteborg). Available online at: https://hal.archives-ouvertes.fr/hal-01375606/file/CARS2016_paper_16.pdf Google Scholar\u000a\u000aJohansson-Stenman, O., and Martinsson, P. (2008). Are some lives more valuable? an ethical preferences approach. J. Health Econ. 27, 739752. doi: 10.1016/j.jhealeco.2007.10.001 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aLin, P. (2016). The ethics of autonomous cars, in The Atlantic. Available online at: https://www.theatlantic.com/technology/archive/2013/10/the-ethics-of-autonomous-cars/280360/ (Accessed October 10, 2016).\u000a\u000aLin, P. (2014). Here's Terrible Idea: Robot Cars with Adjustable Ethics Settings. Wired.com. Available online at: http://www.wired.com/2014/08/heres-a-terrible-idea-robot-cars-with-adjustable-ethics-settings\u000a\u000aLitman, T. (2014). Autonomous vehicle implementation predictions. Victor. Transp. Policy Inst. 28. Google Scholar\u000a\u000aMarcus, G. (2012). Moral Machines. New Yorker Blogs. Available online at: http://www.newyorker.com/news/news-desk/moral-machines (Accessed September 28, 2016). Google Scholar\u000a\u000aMoll, J., de Oliveira-Souza, R., Zahn, R., Grafman, J., and Sinnott-Armstrong, W. (eds.). (2008). Moral Psychology, Vol. 3, The Neuroscience of Morality: Emotion, Brain Disorders, and Development. Cambridge, MA: MIT Press.\u000a\u000aNavarrete, C. D., McDonald, M. M., Mott, M. L., and Asher, B. (2012). Virtual morality: emotion and action in a simulated three-dimensional trolley problem. Emotion 12:364. doi: 10.1037/a0025561 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aPatil, I., Cogoni, C., Zangrando, N., Chittaro, L., and Silani, G. (2014). Affective basis of judgment-behavior discrepancy in virtual experiences of moral dilemmas. Soc. Neurosci. 9, 94107. doi: 10.1080/17470919.2013.870091 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aPaxton, J. M., Ungar, L., and Greene, J. D. (2012). Reflection and reasoning in moral judgment. Cogn. Sci. 36, 163177. doi: 10.1111/j.1551-6709.2011.01210.x PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aSkulmowski, A., Bunge, A., Kaspar, K., and Pipa, G. (2014). Forced-choice decision-making in modified trolley dilemma situations: a virtual reality and eye tracking study. Front. Behav. Neurosci. 8:426. doi: 10.3389/fnbeh.2014.00426 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aSpitzer, M. (2016). Editorial: sollte mein Auto mich umbringen wollen mssen. Das Google-Auto und die Ethik. Nervenheilkunde 35, 451455. Available online at: http://www.schattauer.de/t3page/1214.html?manuscript=26096\u000a\u000aSuter, R. S., and Hertwig, R. (2011). Time and moral judgment. Cognition 119, 454458. doi: 10.1016/j.cognition.2011.01.018 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aTassy, S., Oullier, O., Duclos, Y., Coulon, O., Mancini, J., Deruelle, C., et al. (2012). Disrupting the right prefrontal cortex alters moral judgement. Soc. Cogn. Affect. Neurosci. 7, 282288. doi: 10.1093/scan/nsr008 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aTassy, S., Oullier, O., Mancini, J., and Wicker, B. (2013). Discrepancies between judgment and choice of action in moral dilemmas. Front. Psychol. 4:250. doi: 10.3389/fpsyg.2013.00250 PubMed Abstract | CrossRef Full Text | Google Scholar\u000a\u000aThomson, J. J. (1985). The trolley problem. Yale Law J. 94, 13951415. doi: 10.2307/796133 CrossRef Full Text | Google Scholar\u000a\u000aValdesolo, P., and DeSteno, D. (2006). Manipulations of emotional context shape moral judgment. Psychol. Sci. 17, 476477. doi: 10.1111/j.1467-9280.2006.01731.x PubMed Abstract | CrossRef Full Text | Google Scholar
p587
sS'num_comments'
p588
I45
sS'is_self'
p589
I00
sS'visited'
p590
I00
sS'num_reports'
p591
NsS'is_video'
p592
I00
sS'distinguished'
p593
Nsg27
I00
sb.